{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-04-06T20:17:15.710751Z","iopub.status.busy":"2025-04-06T20:17:15.710415Z","iopub.status.idle":"2025-04-06T20:55:47.447482Z","shell.execute_reply":"2025-04-06T20:55:47.446336Z","shell.execute_reply.started":"2025-04-06T20:17:15.710722Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5 month folders.\n","Found 306242 JSON files in the dataset.\n","Processed 2000 files...\n","Processed 3000 files...\n","Processed 4000 files...\n","Processed 5000 files...\n","Processed 6000 files...\n","Processed 7000 files...\n","Processed 8000 files...\n","Processed 9000 files...\n","Processed 10000 files...\n","Processed 11000 files...\n","Processed 12000 files...\n","Processed 13000 files...\n","Processed 14000 files...\n","Processed 15000 files...\n","Processed 16000 files...\n","Processed 17000 files...\n","Processed 18000 files...\n","Processed 19000 files...\n","Processed 20000 files...\n","Processed 21000 files...\n","Processed 22000 files...\n","Processed 24000 files...\n","Processed 25000 files...\n","Processed 26000 files...\n","Processed 27000 files...\n","Processed 28000 files...\n","Processed 29000 files...\n","Processed 30000 files...\n","Processed 32000 files...\n","Processed 33000 files...\n","Processed 34000 files...\n","Processed 35000 files...\n","Processed 36000 files...\n","Processed 37000 files...\n","Processed 38000 files...\n","Processed 39000 files...\n","Processed 40000 files...\n","Processed 41000 files...\n","Processed 42000 files...\n","Processed 43000 files...\n","Processed 44000 files...\n","Processed 45000 files...\n","Processed 46000 files...\n","Processed 47000 files...\n","Processed 48000 files...\n","Processed 49000 files...\n","Processed 50000 files...\n","Processed 51000 files...\n","Processed 52000 files...\n","Processed 53000 files...\n","Processed 54000 files...\n","Processed 55000 files...\n","Processed 56000 files...\n","Processed 57000 files...\n","Processed 58000 files...\n","Processed 59000 files...\n","Processed 60000 files...\n","Processed 61000 files...\n","Processed 62000 files...\n","Processed 63000 files...\n","Processed 64000 files...\n","Processed 65000 files...\n","Processed 66000 files...\n","Processed 67000 files...\n","Processed 68000 files...\n","Processed 69000 files...\n","Processed 70000 files...\n","Processed 71000 files...\n","Processed 72000 files...\n","Processed 73000 files...\n","Processed 74000 files...\n","Processed 75000 files...\n","Processed 76000 files...\n","Processed 77000 files...\n","Processed 78000 files...\n","Processed 79000 files...\n","Processed 80000 files...\n","Processed 81000 files...\n","Processed 82000 files...\n","Processed 83000 files...\n","Processed 84000 files...\n","Processed 85000 files...\n","Processed 86000 files...\n","Processed 87000 files...\n","Processed 88000 files...\n","Processed 89000 files...\n","Processed 90000 files...\n","Processed 91000 files...\n","Processed 93000 files...\n","Processed 94000 files...\n","Processed 95000 files...\n","Processed 96000 files...\n","Processed 97000 files...\n","Processed 98000 files...\n","Processed 99000 files...\n","Processed 100000 files...\n","Processed 101000 files...\n","Processed 102000 files...\n","Processed 103000 files...\n","Processed 104000 files...\n","Processed 105000 files...\n","Processed 106000 files...\n","Processed 107000 files...\n","Processed 108000 files...\n","Processed 109000 files...\n","Processed 110000 files...\n","Processed 111000 files...\n","Processed 112000 files...\n","Processed 113000 files...\n","Processed 114000 files...\n","Processed 115000 files...\n","Processed 116000 files...\n","Processed 117000 files...\n","Processed 118000 files...\n","Processed 119000 files...\n","Processed 120000 files...\n","Processed 121000 files...\n","Processed 122000 files...\n","Processed 123000 files...\n","Processed 124000 files...\n","Processed 125000 files...\n","Processed 126000 files...\n","Processed 127000 files...\n","Processed 128000 files...\n","Processed 129000 files...\n","Processed 130000 files...\n","Processed 131000 files...\n","Processed 132000 files...\n","Processed 133000 files...\n","Processed 134000 files...\n","Processed 135000 files...\n","Processed 136000 files...\n","Processed 137000 files...\n","Processed 139000 files...\n","Processed 140000 files...\n","Processed 141000 files...\n","Processed 142000 files...\n","Processed 143000 files...\n","Processed 144000 files...\n","Processed 145000 files...\n","Processed 146000 files...\n","Processed 147000 files...\n","Processed 148000 files...\n","Processed 149000 files...\n","Processed 150000 files...\n","Processed 151000 files...\n","Processed 152000 files...\n","Processed 153000 files...\n","Processed 154000 files...\n","Processed 155000 files...\n","Processed 156000 files...\n","Processed 157000 files...\n","Processed 158000 files...\n","Processed 159000 files...\n","Processed 160000 files...\n","Processed 161000 files...\n","Processed 162000 files...\n","Processed 163000 files...\n","Processed 164000 files...\n","Processed 165000 files...\n","Processed 166000 files...\n","Processed 167000 files...\n","Processed 168000 files...\n","Processed 169000 files...\n","Processed 170000 files...\n","Processed 171000 files...\n","Processed 172000 files...\n","Processed 173000 files...\n","Processed 174000 files...\n","Processed 175000 files...\n","Processed 176000 files...\n","Processed 177000 files...\n","Processed 178000 files...\n","Processed 179000 files...\n","Processed 180000 files...\n","Processed 181000 files...\n","Processed 182000 files...\n","Processed 183000 files...\n","Processed 184000 files...\n","Processed 185000 files...\n","Processed 186000 files...\n","Processed 187000 files...\n","Processed 188000 files...\n","Processed 189000 files...\n","Processed 190000 files...\n","Processed 192000 files...\n","Processed 193000 files...\n","Processed 194000 files...\n","Processed 195000 files...\n","Processed 196000 files...\n","Processed 197000 files...\n","Processed 198000 files...\n","Processed 199000 files...\n","Processed 200000 files...\n","Processed 201000 files...\n","Processed 202000 files...\n","Processed 203000 files...\n","Processed 204000 files...\n","Processed 205000 files...\n","Processed 206000 files...\n","Processed 207000 files...\n","Processed 208000 files...\n","Processed 209000 files...\n","Processed 210000 files...\n","Processed 211000 files...\n","Processed 212000 files...\n","Processed 213000 files...\n","Processed 214000 files...\n","Processed 215000 files...\n","Processed 216000 files...\n","Processed 217000 files...\n","Processed 218000 files...\n","Processed 219000 files...\n","Processed 221000 files...\n","Processed 222000 files...\n","Processed 223000 files...\n","Processed 224000 files...\n","Processed 225000 files...\n","Processed 226000 files...\n","Processed 227000 files...\n","Processed 228000 files...\n","Processed 229000 files...\n","Processed 230000 files...\n","Processed 231000 files...\n","Processed 232000 files...\n","Processed 233000 files...\n","Processed 234000 files...\n","Processed 235000 files...\n","Processed 236000 files...\n","Processed 237000 files...\n","Processed 238000 files...\n","Processed 239000 files...\n","Processed 240000 files...\n","Processed 241000 files...\n","Processed 242000 files...\n","Processed 243000 files...\n","Processed 244000 files...\n","Processed 245000 files...\n","Processed 246000 files...\n","Processed 247000 files...\n","Processed 248000 files...\n","Processed 249000 files...\n","Processed 250000 files...\n","Processed 251000 files...\n","Processed 252000 files...\n","Processed 253000 files...\n","Processed 254000 files...\n","Processed 255000 files...\n","Processed 257000 files...\n","Processed 258000 files...\n","Processed 259000 files...\n","Processed 260000 files...\n","Processed 261000 files...\n","Processed 262000 files...\n","Processed 263000 files...\n","Processed 264000 files...\n","Processed 265000 files...\n","Processed 266000 files...\n","Processed 267000 files...\n","Processed 268000 files...\n","Processed 269000 files...\n","Processed 270000 files...\n","Processed 271000 files...\n","Processed 272000 files...\n","Processed 273000 files...\n","Processed 274000 files...\n","Processed 275000 files...\n","Processed 276000 files...\n","Processed 277000 files...\n","Processed 278000 files...\n","Processed 279000 files...\n","Processed 280000 files...\n","Processed 281000 files...\n","Processed 282000 files...\n","Processed 283000 files...\n","Processed 284000 files...\n","Processed 285000 files...\n","Processed 286000 files...\n","Processed 287000 files...\n","Processed 288000 files...\n","Processed 289000 files...\n","Processed 290000 files...\n","Processed 291000 files...\n","Processed 292000 files...\n","Processed 293000 files...\n","Processed 294000 files...\n","Processed 295000 files...\n","Processed 296000 files...\n","Processed 297000 files...\n","Processed 298000 files...\n","Processed 299000 files...\n","Processed 300000 files...\n","Processed 301000 files...\n","Processed 302000 files...\n","Processed 304000 files...\n","Processed 305000 files...\n","Processed 306000 files...\n","Found articles for 168 unique dates.\n","Found 144 valid 10-day windows.\n","Processing window 1/144: 2017-12-31 to 2018-01-09\n","Processing window 2/144: 2018-01-01 to 2018-01-10\n","Processing window 3/144: 2018-01-02 to 2018-01-11\n","Processing window 4/144: 2018-01-03 to 2018-01-12\n","Processing window 5/144: 2018-01-04 to 2018-01-13\n","Processing window 6/144: 2018-01-05 to 2018-01-14\n","Processing window 7/144: 2018-01-06 to 2018-01-15\n","Processing window 8/144: 2018-01-07 to 2018-01-16\n","Processing window 9/144: 2018-01-08 to 2018-01-17\n","Processing window 10/144: 2018-01-09 to 2018-01-18\n","Processing window 11/144: 2018-01-10 to 2018-01-19\n","Processing window 12/144: 2018-01-11 to 2018-01-20\n","Processing window 13/144: 2018-01-12 to 2018-01-21\n","Processing window 14/144: 2018-01-13 to 2018-01-22\n","Processing window 15/144: 2018-01-14 to 2018-01-23\n","Processing window 16/144: 2018-01-15 to 2018-01-24\n","Processing window 17/144: 2018-01-16 to 2018-01-25\n","Processing window 18/144: 2018-01-17 to 2018-01-26\n","Processing window 19/144: 2018-01-18 to 2018-01-27\n","Processing window 20/144: 2018-01-19 to 2018-01-28\n","Processing window 21/144: 2018-01-20 to 2018-01-29\n","Processing window 22/144: 2018-01-21 to 2018-01-30\n","Processing window 23/144: 2018-01-22 to 2018-01-31\n","Processing window 24/144: 2018-01-23 to 2018-02-01\n","Processing window 25/144: 2018-01-24 to 2018-02-02\n","Processing window 26/144: 2018-01-25 to 2018-02-03\n","Processing window 27/144: 2018-01-26 to 2018-02-04\n","Processing window 28/144: 2018-01-27 to 2018-02-05\n","Processing window 29/144: 2018-01-28 to 2018-02-06\n","Processing window 30/144: 2018-01-29 to 2018-02-07\n","Processing window 31/144: 2018-01-30 to 2018-02-08\n","Processing window 32/144: 2018-01-31 to 2018-02-09\n","Processing window 33/144: 2018-02-01 to 2018-02-10\n","Processing window 34/144: 2018-02-02 to 2018-02-11\n","Processing window 35/144: 2018-02-03 to 2018-02-12\n","Processing window 36/144: 2018-02-04 to 2018-02-13\n","Processing window 37/144: 2018-02-05 to 2018-02-14\n","Processing window 38/144: 2018-02-06 to 2018-02-15\n","Processing window 39/144: 2018-02-07 to 2018-02-16\n","Processing window 40/144: 2018-02-08 to 2018-02-17\n","Processing window 41/144: 2018-02-09 to 2018-02-18\n","Processing window 42/144: 2018-02-10 to 2018-02-19\n","Processing window 43/144: 2018-02-11 to 2018-02-20\n","Processing window 44/144: 2018-02-12 to 2018-02-21\n","Processing window 45/144: 2018-02-13 to 2018-02-22\n","Processing window 46/144: 2018-02-14 to 2018-02-23\n","Processing window 47/144: 2018-02-15 to 2018-02-24\n","Processing window 48/144: 2018-02-16 to 2018-02-25\n","Processing window 49/144: 2018-02-17 to 2018-02-26\n","Processing window 50/144: 2018-02-18 to 2018-02-27\n","Processing window 51/144: 2018-02-19 to 2018-02-28\n","Processing window 52/144: 2018-02-20 to 2018-03-01\n","Processing window 53/144: 2018-02-21 to 2018-03-02\n","Processing window 54/144: 2018-02-22 to 2018-03-03\n","Processing window 55/144: 2018-02-23 to 2018-03-04\n","Processing window 56/144: 2018-02-24 to 2018-03-05\n","Processing window 57/144: 2018-02-25 to 2018-03-06\n","Processing window 58/144: 2018-02-26 to 2018-03-07\n","Processing window 59/144: 2018-02-27 to 2018-03-08\n","Processing window 60/144: 2018-02-28 to 2018-03-09\n","Processing window 61/144: 2018-03-01 to 2018-03-10\n","Processing window 62/144: 2018-03-02 to 2018-03-11\n","Processing window 63/144: 2018-03-03 to 2018-03-12\n","Processing window 64/144: 2018-03-04 to 2018-03-13\n","Processing window 65/144: 2018-03-05 to 2018-03-14\n","Processing window 66/144: 2018-03-06 to 2018-03-15\n","Processing window 67/144: 2018-03-07 to 2018-03-16\n","Processing window 68/144: 2018-03-08 to 2018-03-17\n","Processing window 69/144: 2018-03-09 to 2018-03-18\n","Processing window 70/144: 2018-03-10 to 2018-03-19\n","Processing window 71/144: 2018-03-11 to 2018-03-20\n","Processing window 72/144: 2018-03-12 to 2018-03-21\n","Processing window 73/144: 2018-03-13 to 2018-03-22\n","Processing window 74/144: 2018-03-14 to 2018-03-23\n","Processing window 75/144: 2018-03-15 to 2018-03-24\n","Processing window 76/144: 2018-03-16 to 2018-03-25\n","Processing window 77/144: 2018-03-17 to 2018-03-26\n","Processing window 78/144: 2018-03-18 to 2018-03-27\n","Processing window 79/144: 2018-03-19 to 2018-03-28\n","Processing window 80/144: 2018-03-20 to 2018-03-29\n","Processing window 81/144: 2018-03-21 to 2018-03-30\n","Processing window 82/144: 2018-03-22 to 2018-03-31\n","Processing window 83/144: 2018-03-23 to 2018-04-01\n","Processing window 84/144: 2018-03-24 to 2018-04-02\n","Processing window 85/144: 2018-03-25 to 2018-04-03\n","Processing window 86/144: 2018-03-26 to 2018-04-04\n","Processing window 87/144: 2018-03-27 to 2018-04-05\n","Processing window 88/144: 2018-03-28 to 2018-04-06\n","Processing window 89/144: 2018-03-29 to 2018-04-07\n","Processing window 90/144: 2018-03-30 to 2018-04-08\n","Processing window 91/144: 2018-03-31 to 2018-04-09\n","Processing window 92/144: 2018-04-01 to 2018-04-10\n","Processing window 93/144: 2018-04-02 to 2018-04-11\n","Processing window 94/144: 2018-04-03 to 2018-04-12\n","Processing window 95/144: 2018-04-04 to 2018-04-13\n","Processing window 96/144: 2018-04-05 to 2018-04-14\n","Processing window 97/144: 2018-04-06 to 2018-04-15\n","Processing window 98/144: 2018-04-07 to 2018-04-16\n","Processing window 99/144: 2018-04-08 to 2018-04-17\n","Processing window 100/144: 2018-04-09 to 2018-04-18\n","Processing window 101/144: 2018-04-10 to 2018-04-19\n","Processing window 102/144: 2018-04-11 to 2018-04-20\n","Processing window 103/144: 2018-04-12 to 2018-04-21\n","Processing window 104/144: 2018-04-13 to 2018-04-22\n","Processing window 105/144: 2018-04-14 to 2018-04-23\n","Processing window 106/144: 2018-04-15 to 2018-04-24\n","Processing window 107/144: 2018-04-16 to 2018-04-25\n","Processing window 108/144: 2018-04-17 to 2018-04-26\n","Processing window 109/144: 2018-04-18 to 2018-04-27\n","Processing window 110/144: 2018-04-19 to 2018-04-28\n","Processing window 111/144: 2018-04-20 to 2018-04-29\n","Processing window 112/144: 2018-04-21 to 2018-04-30\n","Processing window 113/144: 2018-04-22 to 2018-05-01\n","Processing window 114/144: 2018-04-23 to 2018-05-02\n","Processing window 115/144: 2018-04-24 to 2018-05-03\n","Processing window 116/144: 2018-04-25 to 2018-05-04\n","Processing window 117/144: 2018-04-26 to 2018-05-05\n","Processing window 118/144: 2018-04-27 to 2018-05-06\n","Processing window 119/144: 2018-04-28 to 2018-05-07\n","Processing window 120/144: 2018-04-29 to 2018-05-08\n","Processing window 121/144: 2018-04-30 to 2018-05-09\n","Processing window 122/144: 2018-05-01 to 2018-05-10\n","Processing window 123/144: 2018-05-02 to 2018-05-11\n","Processing window 124/144: 2018-05-03 to 2018-05-12\n","Processing window 125/144: 2018-05-04 to 2018-05-13\n","Processing window 126/144: 2018-05-05 to 2018-05-14\n","Processing window 127/144: 2018-05-06 to 2018-05-15\n","Processing window 128/144: 2018-05-07 to 2018-05-16\n","Processing window 129/144: 2018-05-08 to 2018-05-17\n","Processing window 130/144: 2018-05-09 to 2018-05-18\n","Processing window 131/144: 2018-05-10 to 2018-05-19\n","Processing window 132/144: 2018-05-11 to 2018-05-20\n","Processing window 133/144: 2018-05-12 to 2018-05-21\n","Processing window 134/144: 2018-05-13 to 2018-05-22\n","Processing window 135/144: 2018-05-14 to 2018-05-23\n","Processing window 136/144: 2018-05-15 to 2018-05-24\n","Processing window 137/144: 2018-05-16 to 2018-05-25\n","Processing window 138/144: 2018-05-17 to 2018-05-26\n","Processing window 139/144: 2018-05-18 to 2018-05-27\n","Processing window 140/144: 2018-05-19 to 2018-05-28\n","Processing window 141/144: 2018-05-20 to 2018-05-29\n","Processing window 142/144: 2018-05-21 to 2018-05-30\n","Processing window 143/144: 2018-05-22 to 2018-05-31\n","Processing window 144/144: 2018-05-23 to 2018-06-01\n","Generated 432 total examples.\n","Split distribution: Train: 345, Test: 43, Val: 44\n","Dataset processing complete. Output saved to /kaggle/working/processed_dataset_v2\n"]}],"source":["import os\n","import json\n","import glob\n","import random\n","import re\n","import datetime\n","from collections import defaultdict\n","from datetime import datetime, timedelta\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split\n","\n","# Download NLTK resources\n","nltk.download('punkt', quiet=True)\n","\n","def clean_text(text):\n","    \"\"\"Clean and preprocess the article text\"\"\"\n","    if not text or len(text.strip()) == 0:\n","        return \"\"\n","    \n","    # Remove HTML tags\n","    text = re.sub(r'<.*?>', '', text)\n","    \n","    # Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n","    \n","    # Remove special characters and digits\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    text = re.sub(r'\\d+', ' ', text)\n","    \n","    # Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    \n","    return text\n","\n","def tokenize_and_pad(text, max_tokens=512):\n","    \"\"\"Tokenize text and limit to max_tokens\"\"\"\n","    tokens = word_tokenize(text)\n","    return ' '.join(tokens[:max_tokens])\n","\n","def is_duplicate_or_headline_only(text):\n","    \"\"\"Check if text is duplicate or only headline\"\"\"\n","    if not text:\n","        return True\n","    \n","    # Consider it headline-only if it's very short (less than 20 tokens)\n","    tokens = word_tokenize(text)\n","    return len(tokens) < 20\n","\n","def extract_date_from_published(published_str):\n","    \"\"\"Extract date from published string\"\"\"\n","    try:\n","        # Parse the date part from the published string\n","        date_obj = datetime.strptime(published_str.split('T')[0], '%Y-%m-%d')\n","        return date_obj\n","    except (ValueError, AttributeError, IndexError):\n","        return None\n","\n","def process_dataset(input_directory, output_directory, num_examples_per_window=3, num_windows=150, consecutive_days=10, articles_per_day=10):\n","    \"\"\"Process the financial news dataset\"\"\"\n","    # Create output directory structure\n","    os.makedirs(output_directory, exist_ok=True)\n","    \n","    # Create splits directories\n","    for split in ['train', 'test', 'val']:\n","        split_dir = os.path.join(output_directory, split)\n","        os.makedirs(split_dir, exist_ok=True)\n","    \n","    # Get all month folders\n","    month_folders = []\n","    for root, dirs, files in os.walk(input_directory):\n","        for directory in dirs:\n","            if directory.startswith(\"2018_\"):\n","                month_folders.append(os.path.join(root, directory))\n","    \n","    print(f\"Found {len(month_folders)} month folders.\")\n","    \n","    # Find all JSON files in the month folders\n","    all_json_files = []\n","    for month_folder in month_folders:\n","        for root, dirs, files in os.walk(month_folder):\n","            for file in files:\n","                if file.endswith('.json'):\n","                    all_json_files.append(os.path.join(root, file))\n","    \n","    print(f\"Found {len(all_json_files)} JSON files in the dataset.\")\n","    \n","    # Parse all articles and organize by date\n","    articles_by_date = defaultdict(list)\n","    \n","    for i, json_file in enumerate(all_json_files):\n","        try:\n","            with open(json_file, 'r', encoding='utf-8') as f:\n","                try:\n","                    data = json.load(f)\n","                    \n","                    # Skip if no thread or text\n","                    if 'thread' not in data or 'text' not in data or not data['text']:\n","                        continue\n","                        \n","                    # Extract and clean text\n","                    text = clean_text(data['text'])\n","                    \n","                    # Skip if duplicate or headline only\n","                    if is_duplicate_or_headline_only(text):\n","                        continue\n","                    \n","                    # Get date from published field\n","                    published = data.get('published') or data.get('thread', {}).get('published')\n","                    date_obj = extract_date_from_published(published)\n","                    \n","                    if date_obj:\n","                        # Create article object\n","                        article = {\n","                            'title': data.get('title', ''),\n","                            'text': text,\n","                            'published': published,\n","                            'date': date_obj,\n","                            'source': data.get('thread', {}).get('site', '')\n","                        }\n","                        \n","                        # Add to articles by date\n","                        articles_by_date[date_obj.strftime('%Y-%m-%d')].append(article)\n","                        \n","                except json.JSONDecodeError:\n","                    # Handle HTML or other non-JSON content\n","                    with open(json_file, 'r', encoding='utf-8', errors='ignore') as text_f:\n","                        content = text_f.read(100)  # Read just a bit to check\n","                        if '<html' in content.lower():\n","                            # Skip HTML files\n","                            continue\n","                            \n","        except Exception as e:\n","            print(f\"Error processing file {json_file}: {str(e)}\")\n","        \n","        # Print progress every 1000 files\n","        if (i + 1) % 1000 == 0:\n","            print(f\"Processed {i + 1} files...\")\n","    \n","    print(f\"Found articles for {len(articles_by_date)} unique dates.\")\n","    \n","    # Sort dates\n","    sorted_dates = sorted(articles_by_date.keys())\n","    \n","    if not sorted_dates:\n","        print(\"No valid dates found. Check the dataset structure and file formats.\")\n","        return\n","    \n","    # Find all valid windows of 10 consecutive days with sufficient articles\n","    valid_windows = []\n","    for i in range(len(sorted_dates) - consecutive_days + 1):\n","        window_start = sorted_dates[i]\n","        window_dates = []\n","        current_date = datetime.strptime(window_start, \"%Y-%m-%d\")\n","        \n","        valid_window = True\n","        for j in range(consecutive_days):\n","            date_str = current_date.strftime(\"%Y-%m-%d\")\n","            if date_str in articles_by_date and len(articles_by_date[date_str]) >= articles_per_day:\n","                window_dates.append(date_str)\n","                current_date += timedelta(days=1)\n","            else:\n","                valid_window = False\n","                break\n","        \n","        if valid_window and len(window_dates) == consecutive_days:\n","            valid_windows.append(window_dates)\n","    \n","    print(f\"Found {len(valid_windows)} valid 10-day windows.\")\n","    \n","    if not valid_windows:\n","        print(\"Could not find a valid window of consecutive days with sufficient articles.\")\n","        return\n","    \n","    # Limit to requested number of windows if needed\n","    if len(valid_windows) > num_windows:\n","        valid_windows = random.sample(valid_windows, num_windows)\n","        print(f\"Randomly selected {num_windows} windows.\")\n","    \n","    # Create examples for each window\n","    all_examples = []\n","    example_id = 1\n","    \n","    for window_idx, window_dates in enumerate(valid_windows):\n","        print(f\"Processing window {window_idx+1}/{len(valid_windows)}: {window_dates[0]} to {window_dates[-1]}\")\n","        \n","        # Generate multiple examples for each window\n","        for example_idx in range(num_examples_per_window):\n","            example = {\n","                'id': example_id,\n","                'window_id': window_idx + 1,\n","                'dates': window_dates,\n","                'articles_by_date': {}\n","            }\n","            \n","            # For each date in the window, select random articles\n","            for date in window_dates:\n","                available_articles = articles_by_date[date]\n","                \n","                # Randomly select articles_per_day articles\n","                if len(available_articles) > articles_per_day:\n","                    selected_articles = random.sample(available_articles, articles_per_day)\n","                else:\n","                    selected_articles = available_articles\n","                \n","                # Process and tokenize each article\n","                processed_articles = []\n","                for article in selected_articles:\n","                    processed_text = tokenize_and_pad(article['text'])\n","                    \n","                    if processed_text:  # Only add if there's valid text after processing\n","                        processed_articles.append({\n","                            'title': article['title'],\n","                            'text': processed_text,\n","                            'source': article['source']\n","                        })\n","                \n","                example['articles_by_date'][date] = processed_articles\n","            \n","            all_examples.append(example)\n","            example_id += 1\n","    \n","    print(f\"Generated {len(all_examples)} total examples.\")\n","    \n","    # Split examples into train/test/val\n","    random.shuffle(all_examples)  # Shuffle to ensure random distribution\n","    \n","    # Calculate split sizes\n","    total = len(all_examples)\n","    train_size = int(total * 0.8)\n","    test_size = int(total * 0.1)\n","    val_size = total - train_size - test_size\n","    \n","    # Split examples\n","    train_examples = all_examples[:train_size]\n","    test_examples = all_examples[train_size:train_size+test_size]\n","    val_examples = all_examples[train_size+test_size:]\n","    \n","    print(f\"Split distribution: Train: {len(train_examples)}, Test: {len(test_examples)}, Val: {len(val_examples)}\")\n","    \n","    # Write examples to files\n","    splits = {\n","        'train': train_examples,\n","        'test': test_examples,\n","        'val': val_examples\n","    }\n","    \n","    for split_name, examples in splits.items():\n","        split_dir = os.path.join(output_directory, split_name)\n","        \n","        for example in examples:\n","            # Create a directory for this example\n","            example_dir = os.path.join(split_dir, f\"example{example['id']}\")\n","            os.makedirs(example_dir, exist_ok=True)\n","            \n","            # Create text files for each date in the window\n","            for date, articles in example['articles_by_date'].items():\n","                date_file = os.path.join(example_dir, f\"{date}.txt\")\n","                \n","                with open(date_file, 'w', encoding='utf-8') as f:\n","                    for idx, article in enumerate(articles):\n","                        f.write(f\"--- Article {idx+1} ---\\n\")\n","                        f.write(f\"Title: {article['title']}\\n\")\n","                        f.write(f\"Source: {article['source']}\\n\")\n","                        f.write(f\"Date: {date}\\n\")\n","                        f.write(\"Text:\\n\")\n","                        f.write(article['text'])\n","                        f.write(\"\\n\\n\")\n","    \n","    print(f\"Dataset processing complete. Output saved to {output_directory}\")\n","\n","if __name__ == \"__main__\":\n","    # Set the paths for Kaggle environment\n","    input_directory = \"/kaggle/input/us-financial-news-articles\"  # Path to the dataset with month folders\n","    output_directory = \"/kaggle/working/processed_dataset_v2\"        # Path for output in Kaggle\n","    \n","    # Process the dataset\n","    process_dataset(\n","        input_directory=input_directory,\n","        output_directory=output_directory,\n","        num_examples_per_window=3,    # Number of examples per window\n","        num_windows=150,              # Max number of windows to use (can be less if not enough found)\n","        consecutive_days=10,          # Number of consecutive days in each window\n","        articles_per_day=10           # Number of articles to select per day\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":49948,"sourceId":90823,"sourceType":"datasetVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
