{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/results.zip -d /content/data"
      ],
      "metadata": {
        "id": "66pbAmHzQdWB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5nxxmqPLcQW",
        "outputId": "95688ecd-c8cc-4fbd-c3b0-73b8043ebf17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading stock price data...\n",
            "CSV date range: 2018-01-02 → 2018-05-30\n",
            "✅ All required stock columns are present.\n",
            "Model initialized.\n",
            "\n",
            "Processing train split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train: 100%|██████████| 345/345 [03:48<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train dataset with 345 examples\n",
            "Generating embeddings for train...\n",
            "Saved train embeddings with shape torch.Size([345, 9, 312]) to /content/data/preprocessed/train_embeddings_9x312.pt\n",
            "\n",
            "Processing val split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing val: 100%|██████████| 44/44 [00:28<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved val dataset with 44 examples\n",
            "Generating embeddings for val...\n",
            "Saved val embeddings with shape torch.Size([44, 9, 312]) to /content/data/preprocessed/val_embeddings_9x312.pt\n",
            "\n",
            "Processing test split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test: 100%|██████████| 43/43 [00:28<00:00,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved test dataset with 43 examples\n",
            "Generating embeddings for test...\n",
            "Saved test embeddings with shape torch.Size([43, 9, 312]) to /content/data/preprocessed/test_embeddings_9x312.pt\n",
            "\n",
            "=== Dataset Summary ===\n",
            "Train split:\n",
            "  • Examples: 345\n",
            "  • News tensor: torch.Size([345, 9, 10, 768])\n",
            "  • Metrics tensor: torch.Size([345, 9, 5, 5])\n",
            "  • Targets tensor: torch.Size([345, 25])\n",
            "  • Embeddings tensor: torch.Size([345, 9, 312])\n",
            "Val split:\n",
            "  • Examples: 44\n",
            "  • News tensor: torch.Size([44, 9, 10, 768])\n",
            "  • Metrics tensor: torch.Size([44, 9, 5, 5])\n",
            "  • Targets tensor: torch.Size([44, 25])\n",
            "  • Embeddings tensor: torch.Size([44, 9, 312])\n",
            "Test split:\n",
            "  • Examples: 43\n",
            "  • News tensor: torch.Size([43, 9, 10, 768])\n",
            "  • Metrics tensor: torch.Size([43, 9, 5, 5])\n",
            "  • Targets tensor: torch.Size([43, 25])\n",
            "  • Embeddings tensor: torch.Size([43, 9, 312])\n",
            "\n",
            "✅ Processing complete. Generated [batch×9×312] embeddings for all splits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import ast\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# === Device Setup ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# === FinBERT Model ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "finbert_model = AutoModel.from_pretrained(\"yiyanghkust/finbert-tone\").to(device)\n",
        "finbert_model.eval()\n",
        "\n",
        "# === Define 1D Convolution Network for Financial Metrics ===\n",
        "class FinancialConvNet(nn.Module):\n",
        "    def __init__(self, in_channels=5, kernel_size=3, hidden_dim=64, stocks_count=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.stocks_count = stocks_count\n",
        "\n",
        "        # Separate convolution layer for each metric\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=stocks_count,     # Each stock is a channel\n",
        "                out_channels=hidden_dim,      # Hidden dimension per metric\n",
        "                kernel_size=kernel_size,      # Kernel size of 3\n",
        "                padding=kernel_size//2        # Same padding\n",
        "            ) for _ in range(in_channels)\n",
        "        ])\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # Dimension reducer after convolution\n",
        "        self.metrics_dim_reducer = nn.Linear(hidden_dim * in_channels, 384)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_length, metrics, stocks]\n",
        "        batch_size, seq_len, metrics, stocks = x.shape\n",
        "\n",
        "        # Transpose to get format suitable for convolution across days\n",
        "        # We need: [batch_size, metrics, stocks, seq_length]\n",
        "        x_transposed = x.permute(0, 2, 3, 1)\n",
        "\n",
        "        # Process each day's output separately to maintain temporal information\n",
        "        all_days_features = []\n",
        "\n",
        "        for day_idx in range(seq_len):\n",
        "            day_features = []\n",
        "\n",
        "            # Process each metric\n",
        "            for metric_idx in range(metrics):\n",
        "                # Get all stocks data for this metric across all days\n",
        "                # Shape: [batch_size, stocks, seq_length]\n",
        "                metric_data = x_transposed[:, metric_idx]\n",
        "\n",
        "                # Apply convolution\n",
        "                # This will give us: [batch_size, hidden_dim, seq_length]\n",
        "                conv_output = self.relu(self.conv_layers[metric_idx](metric_data))\n",
        "\n",
        "                # Extract features for the current day from the convolution output\n",
        "                # Shape: [batch_size, hidden_dim]\n",
        "                day_metric_features = conv_output[:, :, day_idx]\n",
        "\n",
        "                day_features.append(day_metric_features)\n",
        "\n",
        "            # Combine all metric features for this day\n",
        "            # Shape: [batch_size, metrics*hidden_dim]\n",
        "            combined_day_features = torch.cat(day_features, dim=1)\n",
        "\n",
        "            # Apply dimension reduction\n",
        "            # Shape: [batch_size, 384]\n",
        "            day_output = self.tanh(self.metrics_dim_reducer(combined_day_features))\n",
        "\n",
        "            all_days_features.append(day_output)\n",
        "\n",
        "        # Stack all days features\n",
        "        # Shape: [batch_size, seq_length, 384]\n",
        "        return torch.stack(all_days_features, dim=1)\n",
        "\n",
        "# === News Processing Module ===\n",
        "class NewsProcessor(nn.Module):\n",
        "    def __init__(self, input_dim=768, output_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Weighted sum to reduce dimensionality\n",
        "        self.attention = nn.Linear(input_dim, 1)\n",
        "        self.feature_projection = nn.Linear(input_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_length, num_articles, embed_dim]\n",
        "        batch_size, seq_len, num_articles, embed_dim = x.shape\n",
        "\n",
        "        all_days_features = []\n",
        "\n",
        "        for day in range(seq_len):\n",
        "            # Extract current day's articles\n",
        "            # Shape: [batch_size, num_articles, embed_dim]\n",
        "            day_articles = x[:, day]\n",
        "\n",
        "            # Calculate attention weights\n",
        "            # Shape: [batch_size, num_articles, 1]\n",
        "            attn_weights = F.softmax(self.attention(day_articles), dim=1)\n",
        "\n",
        "            # Apply attention weights (weighted sum)\n",
        "            # Shape: [batch_size, embed_dim]\n",
        "            weighted_features = torch.sum(day_articles * attn_weights, dim=1)\n",
        "\n",
        "            # Project to output dimension\n",
        "            # Shape: [batch_size, output_dim]\n",
        "            day_features = self.tanh(self.feature_projection(weighted_features))\n",
        "            all_days_features.append(day_features)\n",
        "\n",
        "        # Stack to get [batch_size, seq_len, output_dim]\n",
        "        return torch.stack(all_days_features, dim=1)\n",
        "\n",
        "# === Combined Model with Preserved Temporal Information ===\n",
        "class CombinedFinancialModel(nn.Module):\n",
        "    def __init__(self, finbert_dim=768, metrics_count=5, stocks_count=5, output_dim=312, pred_metrics=5):\n",
        "        super().__init__()\n",
        "\n",
        "        # News processing\n",
        "        self.news_processor = NewsProcessor(input_dim=finbert_dim, output_dim=512)\n",
        "\n",
        "        # Metrics processing\n",
        "        self.conv_net = FinancialConvNet(in_channels=metrics_count, kernel_size=3, stocks_count=stocks_count)\n",
        "\n",
        "        # Combined processing\n",
        "        self.concat_reducer = nn.Linear(512 + 384, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # Prediction layer for next day (day 10)\n",
        "        self.predictor = nn.Linear(output_dim * 9, pred_metrics * stocks_count)\n",
        "\n",
        "    def forward(self, news_embeddings, metrics_data):\n",
        "        # Process news through attention and projection\n",
        "        # news_embeddings shape: [batch_size, 9 days, 10 articles, 768]\n",
        "        # Output shape: [batch_size, 9 days, 512]\n",
        "        news_features = self.news_processor(news_embeddings)\n",
        "\n",
        "        # Process metrics through convolutions\n",
        "        # metrics_data shape: [batch_size, 9 days, 5 metrics, 5 stocks]\n",
        "        # Output shape: [batch_size, 9 days, 384]\n",
        "        metrics_features = self.conv_net(metrics_data)\n",
        "\n",
        "        # Concatenate along feature dimension\n",
        "        # Shape: [batch_size, 9 days, 512+384]\n",
        "        combined = torch.cat([news_features, metrics_features], dim=2)\n",
        "\n",
        "        # Apply linear and tanh to match diagram\n",
        "        # Shape: [batch_size, 9 days, output_dim]\n",
        "        temporal_features = self.tanh(self.concat_reducer(combined))\n",
        "\n",
        "        # For prediction, flatten the sequence dimension to predict day 10\n",
        "        # Shape: [batch_size, 9*output_dim]\n",
        "        flat_features = temporal_features.reshape(temporal_features.shape[0], -1)\n",
        "\n",
        "        # Predict next day's metrics for all stocks\n",
        "        # Shape: [batch_size, pred_metrics*stocks_count]\n",
        "        predictions = self.predictor(flat_features)\n",
        "\n",
        "        return {\n",
        "            'temporal_features': temporal_features,  # Intermediate features preserving temporal structure\n",
        "            'predictions': predictions  # Predictions for day 10\n",
        "        }\n",
        "\n",
        "# === Data Processing Functions ===\n",
        "def find_nearest_date(df, target_date, max_days=7):\n",
        "    \"\"\"Find closest valid date within business days.\"\"\"\n",
        "    dates = df.index\n",
        "    if target_date in dates:\n",
        "        return target_date\n",
        "\n",
        "    candidates = dates[(dates >= target_date - pd.Timedelta(days=max_days)) &\n",
        "                      (dates <= target_date + pd.Timedelta(days=max_days))]\n",
        "\n",
        "    if not candidates.empty:\n",
        "        return candidates[np.argmin(np.abs((candidates - target_date).total_seconds()))]\n",
        "\n",
        "    return None\n",
        "\n",
        "def process_article_folder(example_path):\n",
        "    \"\"\"Process 9 days of news articles, 10 articles per day\"\"\"\n",
        "    day_files = sorted([f for f in os.listdir(example_path) if f.endswith(\".txt\")])[:9]  # Process 9 days\n",
        "    daily_embeddings = []\n",
        "\n",
        "    for day_file in day_files:\n",
        "        day_path = os.path.join(example_path, day_file)\n",
        "\n",
        "        with open(day_path, 'r', encoding='utf-8') as f:\n",
        "            articles = re.split(r'--- Article \\d+ ---', f.read())\n",
        "            articles = [a.strip() for a in articles if a.strip()]\n",
        "\n",
        "            # Select up to 10 random articles\n",
        "            if len(articles) > 10:\n",
        "                articles = np.random.choice(articles, 10, replace=False).tolist()\n",
        "            elif len(articles) < 10:\n",
        "                # Pad with empty strings if fewer than 10 articles\n",
        "                articles = articles + [''] * (10 - len(articles))\n",
        "\n",
        "        # Batch processing\n",
        "        inputs = tokenizer(\n",
        "            articles,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                outputs = finbert_model(**inputs)\n",
        "\n",
        "        # Get the CLS token embeddings from last hidden state\n",
        "        # Shape: [10, 768]\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        daily_embeddings.append(cls_embeddings)\n",
        "\n",
        "    return torch.stack(daily_embeddings)  # Shape: [9, 10, 768]\n",
        "\n",
        "def extract_raw_metrics(df, start_date, stocks, metrics, include_target=False):\n",
        "    \"\"\"Extract 9 days of metrics data and optionally the 10th day as target\"\"\"\n",
        "    days_to_extract = 10 if include_target else 9\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(days_to_extract)]\n",
        "    valid_dates = [find_nearest_date(df, d) for d in date_range]\n",
        "\n",
        "    metrics_data = []\n",
        "    for date in valid_dates[:9]:  # First 9 days are input features\n",
        "        if date is None:\n",
        "            return None, None, None\n",
        "\n",
        "        daily_data = []\n",
        "        for stock in stocks:\n",
        "            try:\n",
        "                stock_metrics = [df.loc[date, (metric, stock)] for metric in metrics]\n",
        "            except KeyError:\n",
        "                return None, None, None\n",
        "            daily_data.append(stock_metrics)\n",
        "\n",
        "        daily_tensor = torch.tensor(daily_data, dtype=torch.float32).T\n",
        "        metrics_data.append(daily_tensor)\n",
        "\n",
        "    input_tensor = torch.stack(metrics_data)  # Shape: (9, 5 metrics, 5 stocks)\n",
        "\n",
        "    # If target day is requested, extract it separately\n",
        "    target_tensor = None\n",
        "    target_date = None\n",
        "    if include_target and len(valid_dates) == 10 and valid_dates[9] is not None:\n",
        "        target_date = valid_dates[9]\n",
        "        target_data = []\n",
        "        for stock in stocks:\n",
        "            try:\n",
        "                stock_metrics = [df.loc[valid_dates[9], (metric, stock)] for metric in metrics]\n",
        "                target_data.append(stock_metrics)\n",
        "            except KeyError:\n",
        "                return input_tensor, None, None\n",
        "\n",
        "        target_tensor = torch.tensor(target_data, dtype=torch.float32).flatten()\n",
        "\n",
        "    return input_tensor, target_tensor, target_date\n",
        "\n",
        "# === Generate Dataset with Dates ===\n",
        "def generate_dataset_with_dates(df, input_dir, output_dir, split):\n",
        "    \"\"\"Generate dataset with inputs, targets, and dates\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    example_folders = sorted(os.listdir(input_dir))\n",
        "    all_news_tensors = []\n",
        "    all_metrics_tensors = []\n",
        "    all_targets = []\n",
        "    all_dates = []  # To store date information\n",
        "    valid_examples = []\n",
        "\n",
        "    stocks = ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX']\n",
        "    metrics = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "    for folder in tqdm(example_folders, desc=f\"Processing {split}\"):\n",
        "        example_path = os.path.join(input_dir, folder)\n",
        "        if not os.path.isdir(example_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            txt_files = sorted([f for f in os.listdir(example_path) if f.endswith(\".txt\")])\n",
        "            if len(txt_files) < 9:\n",
        "                continue\n",
        "\n",
        "            start_date_str = txt_files[0].replace(\".txt\", \"\")\n",
        "            start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
        "\n",
        "            # Process news\n",
        "            news_tensor = process_article_folder(example_path)\n",
        "\n",
        "            # Process metrics and extract target (10th day)\n",
        "            metrics_tensor, target_tensor, target_date = extract_raw_metrics(\n",
        "                df, start_date, stocks, metrics, include_target=True\n",
        "            )\n",
        "\n",
        "            if metrics_tensor is None or target_tensor is None or target_date is None:\n",
        "                continue\n",
        "\n",
        "            # Collect data\n",
        "            all_news_tensors.append(news_tensor)\n",
        "            all_metrics_tensors.append(metrics_tensor)\n",
        "            all_targets.append(target_tensor)\n",
        "\n",
        "            # Store date information\n",
        "            date_info = {\n",
        "                'start_date': start_date.strftime(\"%Y-%m-%d\"),\n",
        "                'target_date': target_date.strftime(\"%Y-%m-%d\") if target_date else None,\n",
        "                'example_id': folder\n",
        "            }\n",
        "            all_dates.append(date_info)\n",
        "            valid_examples.append(folder)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed processing {folder}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Save dataset if we have examples\n",
        "    if all_news_tensors and all_metrics_tensors and all_targets:\n",
        "        combined_news = torch.stack(all_news_tensors)\n",
        "        combined_metrics = torch.stack(all_metrics_tensors)\n",
        "        combined_targets = torch.stack(all_targets)\n",
        "\n",
        "        # Save tensors\n",
        "        torch.save(combined_news.cpu(), os.path.join(output_dir, f\"{split}_news.pt\"))\n",
        "        torch.save(combined_metrics.cpu(), os.path.join(output_dir, f\"{split}_metrics.pt\"))\n",
        "        torch.save(combined_targets.cpu(), os.path.join(output_dir, f\"{split}_targets.pt\"))\n",
        "\n",
        "        # Save date information\n",
        "        dates_df = pd.DataFrame(all_dates)\n",
        "        dates_df.to_csv(os.path.join(output_dir, f\"{split}_dates.csv\"), index=False)\n",
        "\n",
        "        print(f\"Saved {split} dataset with {len(valid_examples)} examples\")\n",
        "        return combined_news, combined_metrics, combined_targets, dates_df\n",
        "\n",
        "    return None, None, None, None\n",
        "\n",
        "# === Generate Embeddings Function ===\n",
        "def generate_and_save_embeddings(model, news_tensors, metrics_tensors, dates_df, output_dir, split):\n",
        "    \"\"\"\n",
        "    Generate concatenated embeddings with dimension [batch×9×312] and save them\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    batch_size = news_tensors.shape[0]\n",
        "    max_batch = 16  # Adjust based on GPU memory\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, batch_size, max_batch):\n",
        "            news_batch = news_tensors[i:i+max_batch].to(device)\n",
        "            metrics_batch = metrics_tensors[i:i+max_batch].to(device)\n",
        "\n",
        "            # Forward pass to get temporal features (before flattening)\n",
        "            outputs = model(news_batch, metrics_batch)\n",
        "            temporal_features = outputs['temporal_features']  # Shape: [batch, 9, 312]\n",
        "\n",
        "            all_embeddings.append(temporal_features.cpu())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    combined_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "    # Save the embeddings\n",
        "    embedding_path = os.path.join(output_dir, f\"{split}_embeddings_9x312.pt\")\n",
        "    torch.save(combined_embeddings, embedding_path)\n",
        "\n",
        "    print(f\"Saved {split} embeddings with shape {combined_embeddings.shape} to {embedding_path}\")\n",
        "\n",
        "    return combined_embeddings\n",
        "\n",
        "# === Main Execution Function ===\n",
        "def main():\n",
        "    # Set paths\n",
        "    csv_path = \"/content/cleaned_stock_prices.csv\"\n",
        "    data_dir = \"/content/data/processed_dataset_v2\"\n",
        "    output_dir = \"/content/data/preprocessed\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load CSV with multi-index columns\n",
        "    print(\"Loading stock price data...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Set first column as index\n",
        "    df.set_index(df.columns[0], inplace=True)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Parse the column headers properly\n",
        "    column_pairs = []\n",
        "    for col in df.columns:\n",
        "        # Extract components from string like \"('Open', 'AAPL')\"\n",
        "        match = re.match(r\"\\('([^']+)', '([^']+)'\\)\", col)\n",
        "        if match:\n",
        "            metric, stock = match.groups()\n",
        "            column_pairs.append((metric, stock))\n",
        "        else:\n",
        "            column_pairs.append(col)\n",
        "\n",
        "    # Create MultiIndex columns\n",
        "    df.columns = pd.MultiIndex.from_tuples(column_pairs)\n",
        "\n",
        "    # Convert index to datetime if needed\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "        df.dropna(subset=[df.index.name], inplace=True)\n",
        "\n",
        "    # Check date range\n",
        "    print(f\"CSV date range: {df.index.min().date()} → {df.index.max().date()}\")\n",
        "\n",
        "    # Define constants\n",
        "    stocks = ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX']\n",
        "    metrics = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "    # === Column Verification ===\n",
        "    expected_cols = [(m, s) for m in metrics for s in stocks]\n",
        "    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing critical columns: {missing_cols}\")\n",
        "    else:\n",
        "        print(\"✅ All required stock columns are present.\")\n",
        "\n",
        "    # Initialize model\n",
        "    combined_model = CombinedFinancialModel().to(device)\n",
        "    print(\"Model initialized.\")\n",
        "\n",
        "    # Process all splits and generate datasets with dates\n",
        "    all_data = {}\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        print(f\"\\nProcessing {split} split...\")\n",
        "        input_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        # Generate dataset with dates\n",
        "        news_tensors, metrics_tensors, targets, dates_df = generate_dataset_with_dates(\n",
        "            df, input_dir, output_dir, split\n",
        "        )\n",
        "\n",
        "        if news_tensors is not None:\n",
        "            # Store for later use\n",
        "            all_data[split] = {\n",
        "                'news': news_tensors,\n",
        "                'metrics': metrics_tensors,\n",
        "                'targets': targets,\n",
        "                'dates': dates_df\n",
        "            }\n",
        "\n",
        "            # Generate and save embeddings\n",
        "            print(f\"Generating embeddings for {split}...\")\n",
        "            embeddings = generate_and_save_embeddings(\n",
        "                combined_model, news_tensors, metrics_tensors, dates_df, output_dir, split\n",
        "            )\n",
        "            all_data[split]['embeddings'] = embeddings\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n=== Dataset Summary ===\")\n",
        "    for split, data in all_data.items():\n",
        "        print(f\"{split.capitalize()} split:\")\n",
        "        print(f\"  • Examples: {data['news'].shape[0]}\")\n",
        "        print(f\"  • News tensor: {data['news'].shape}\")\n",
        "        print(f\"  • Metrics tensor: {data['metrics'].shape}\")\n",
        "        print(f\"  • Targets tensor: {data['targets'].shape}\")\n",
        "        print(f\"  • Embeddings tensor: {data['embeddings'].shape}\")\n",
        "\n",
        "    print(\"\\n✅ Processing complete. Generated [batch×9×312] embeddings for all splits.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYltTygYUk6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}