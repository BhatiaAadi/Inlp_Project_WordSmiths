{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/results.zip -d /content/data"
      ],
      "metadata": {
        "id": "I1sLm4uI6qtG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to extract the 10th date from an example folder\n",
        "def extract_tenth_date(example_path):\n",
        "    dates = []\n",
        "    for file in os.listdir(example_path):\n",
        "        if file.endswith('.txt'):\n",
        "            date_str = file.split('.')[0]  # Extract YYYY-MM-DD from filename\n",
        "            dates.append(date_str)\n",
        "\n",
        "    # Sort dates chronologically\n",
        "    dates.sort()\n",
        "\n",
        "    # Return the 10th date if available\n",
        "    if len(dates) >= 10:\n",
        "        return dates[9]  # 0-indexed, so the 10th element is at index 9\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to process all examples in a directory\n",
        "def process_directory(directory_path, stock_data):\n",
        "    results = []\n",
        "\n",
        "    for example_folder in os.listdir(directory_path):\n",
        "        example_path = os.path.join(directory_path, example_folder)\n",
        "        if os.path.isdir(example_path):\n",
        "            tenth_date = extract_tenth_date(example_path)\n",
        "            if tenth_date and tenth_date in stock_data.index:\n",
        "                results.append(stock_data.loc[tenth_date])\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Main execution\n",
        "def main(all_stock_path, train_dir, test_dir, val_dir):\n",
        "    # Load stock data\n",
        "    stock_data = pd.read_csv(all_stock_path, index_col=0)\n",
        "\n",
        "    # Process each directory\n",
        "    train_data = process_directory(train_dir, stock_data)\n",
        "    test_data = process_directory(test_dir, stock_data)\n",
        "    val_data = process_directory(val_dir, stock_data)\n",
        "\n",
        "    # Save results\n",
        "    train_data.to_csv('train.csv')\n",
        "    test_data.to_csv('test.csv')\n",
        "    val_data.to_csv('val.csv')\n",
        "\n",
        "    print(f\"Generated train.csv with {len(train_data)} entries\")\n",
        "    print(f\"Generated test.csv with {len(test_data)} entries\")\n",
        "    print(f\"Generated val.csv with {len(val_data)} entries\")\n",
        "\n",
        "# Replace these paths with your actual paths\n",
        "all_stock_path = '/content/stock_prices_complete.csv'\n",
        "train_dir = '/content/data/processed_dataset_v2/train'\n",
        "test_dir = '/content/data/processed_dataset_v2/test'\n",
        "val_dir = '/content/data/processed_dataset_v2/val'\n",
        "\n",
        "main(all_stock_path, train_dir, test_dir, val_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-S0WhKMQvjV",
        "outputId": "a1a09ef5-7419-4c9d-cbab-37a8a04073ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated train.csv with 340 entries\n",
            "Generated test.csv with 42 entries\n",
            "Generated val.csv with 44 entries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "import re\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Linear layer with Low-Rank Adaptation (LoRA)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, r=8, alpha=16):\n",
        "        super().__init__()\n",
        "        self.original = nn.Linear(in_features, out_features)\n",
        "        self.lora_A = nn.Parameter(torch.zeros(in_features, r))\n",
        "        self.lora_B = nn.Parameter(torch.zeros(r, out_features))\n",
        "        self.scaling = alpha / r\n",
        "        self.r = r\n",
        "        # Initialize weights for LoRA\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Original path\n",
        "        original_output = self.original(x)\n",
        "        # LoRA path\n",
        "        lora_output = (x @ self.lora_A) @ self.lora_B\n",
        "        # Combine with scaling\n",
        "        return original_output + (lora_output * self.scaling)\n",
        "\n",
        "\n",
        "class LoRATransformerWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrap a transformer model with LoRA adaptation in attention layers\n",
        "    \"\"\"\n",
        "    def __init__(self, model, r=8, alpha=16):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.apply_lora()\n",
        "\n",
        "    def apply_lora(self):\n",
        "        \"\"\"\n",
        "        Apply LoRA to the query and value projection layers in attention blocks\n",
        "        \"\"\"\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.Linear) and any(key in name for key in ['query', 'value']):\n",
        "                in_features, out_features = module.in_features, module.out_features\n",
        "                parent_name = '.'.join(name.split('.')[:-1])\n",
        "                layer_name = name.split('.')[-1]\n",
        "\n",
        "                # Create LoRA layer\n",
        "                lora_layer = LoRALinear(in_features, out_features, r=self.r, alpha=self.alpha)\n",
        "                # Copy weights from original layer\n",
        "                lora_layer.original.weight.data = module.weight.data.clone()\n",
        "                if module.bias is not None:\n",
        "                    lora_layer.original.bias.data = module.bias.data.clone()\n",
        "\n",
        "                # Set the LoRA layer in the parent module\n",
        "                parent = self.model\n",
        "                for part in parent_name.split('.'):\n",
        "                    if part:\n",
        "                        parent = getattr(parent, part)\n",
        "                setattr(parent, layer_name, lora_layer)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.model(*args, **kwargs)\n",
        "\n",
        "\n",
        "class TinyBERTStockPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Stock prediction model using TinyBERT with LoRA\n",
        "    Takes 9-day temporal embeddings of shape [batch_size, 9, 312]\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=312, hidden_dim=312, output_dim=30, lora_r=8, lora_alpha=16):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load TinyBERT model\n",
        "        self.bert_config = AutoConfig.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
        "        self.bert = AutoModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
        "\n",
        "        # Apply LoRA to TinyBERT\n",
        "        self.bert = LoRATransformerWrapper(self.bert, r=lora_r, alpha=lora_alpha)\n",
        "\n",
        "        # Output projection layers\n",
        "        bert_output_dim = self.bert_config.hidden_size  # 312\n",
        "        self.output_block = nn.Sequential(\n",
        "            nn.Linear(bert_output_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape [batch_size, 9, 312]\n",
        "                             (batch, days, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted values for all metrics/stocks, shape [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, embed_dim = x.shape  # batch, 9, 312\n",
        "\n",
        "        # Create attention mask (all 1s as we want to attend to all tokens)\n",
        "        attention_mask = torch.ones(batch_size, seq_len, device=x.device)\n",
        "\n",
        "        # Pass through TinyBERT\n",
        "        bert_output = self.bert(inputs_embeds=x, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.pooler_output  # [batch, 312]\n",
        "\n",
        "        # Pass through output layers\n",
        "        output = self.output_block(pooled_output)  # [batch, output_dim]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class StockDataset:\n",
        "    \"\"\"\n",
        "    Dataset for loading pre-processed embeddings and matching them with target values\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings_path, stock_csv_path, metrics_count=6, stocks_count=5):\n",
        "        \"\"\"\n",
        "        Initialize with paths to embeddings and stock data\n",
        "\n",
        "        Args:\n",
        "            embeddings_path (str): Path to embeddings tensor file (.pt)\n",
        "            stock_csv_path (str): Path to CSV with target stock values for 10th day\n",
        "            metrics_count (int): Number of metrics per stock (OHLCV + Adj Close = 6)\n",
        "            stocks_count (int): Number of stocks\n",
        "        \"\"\"\n",
        "        self.embeddings_path = embeddings_path\n",
        "        self.stock_csv_path = stock_csv_path\n",
        "        self.metrics_count = metrics_count\n",
        "        self.stocks_count = stocks_count\n",
        "\n",
        "        # Stock order mapping\n",
        "        self.stock_order = ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX']\n",
        "\n",
        "        # Load the data\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Load embeddings and stock data, then create target tensors\n",
        "        \"\"\"\n",
        "        print(f\"Loading embeddings from {self.embeddings_path}\")\n",
        "        self.embeddings = torch.load(self.embeddings_path)\n",
        "\n",
        "        print(f\"Loading stock data from {self.stock_csv_path}\")\n",
        "        self.stock_df = self.load_stock_csv()\n",
        "\n",
        "        # Create target tensors from stock CSV\n",
        "        self.create_targets()\n",
        "\n",
        "    def load_stock_csv(self):\n",
        "        \"\"\"Load and preprocess stock CSV file with target values\"\"\"\n",
        "        print(f\"Loading stock price data from {self.stock_csv_path}...\")\n",
        "\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(self.stock_csv_path)\n",
        "\n",
        "        print(\"CSV structure sample:\")\n",
        "        print(df.head(1))\n",
        "        print(f\"CSV columns: {df.columns.tolist()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_targets(self):\n",
        "        \"\"\"\n",
        "        Create target tensors based on the specific CSV format with numbered columns\n",
        "        Format: [Metric], [Metric].1, [Metric].2, etc. for each company\n",
        "        \"\"\"\n",
        "        print(\"Creating target tensors...\")\n",
        "\n",
        "        # Base metrics in your CSV\n",
        "        base_metrics = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "        # List to store all targets\n",
        "        all_targets = []\n",
        "\n",
        "        # Process each row in the dataframe\n",
        "        for idx, row in self.stock_df.iterrows():\n",
        "            # Extract target values for this row\n",
        "            row_targets = []\n",
        "\n",
        "            # For each stock (in order)\n",
        "            for stock_idx in range(self.stocks_count):\n",
        "                # For each metric\n",
        "                for metric in base_metrics:\n",
        "                    # For the first stock, column name is just the metric\n",
        "                    # For subsequent stocks, column name is metric.1, metric.2, etc.\n",
        "                    if stock_idx == 0:\n",
        "                        col_name = metric\n",
        "                    else:\n",
        "                        col_name = f\"{metric}.{stock_idx}\"\n",
        "\n",
        "                    # Get the value, with error handling\n",
        "                    try:\n",
        "                        value = float(row[col_name])\n",
        "                        row_targets.append(value)\n",
        "                    except (KeyError, ValueError) as e:\n",
        "                        print(f\"Error extracting {col_name} (for {self.stock_order[stock_idx]}): {e}\")\n",
        "                        # Use 0.0 as a fallback value\n",
        "                        row_targets.append(0.0)\n",
        "\n",
        "            all_targets.append(row_targets)\n",
        "\n",
        "        if all_targets:\n",
        "            self.targets = torch.tensor(all_targets, dtype=torch.float32)\n",
        "            print(f\"Created {len(all_targets)} targets with shape {self.targets.shape}\")\n",
        "\n",
        "            # Make sure we have the same number of targets as embeddings\n",
        "            if len(all_targets) != self.embeddings.shape[0]:\n",
        "                print(f\"Warning: Number of targets ({len(all_targets)}) doesn't match number of embeddings ({self.embeddings.shape[0]})\")\n",
        "\n",
        "                # Take the minimum number to ensure alignment\n",
        "                min_length = min(len(all_targets), self.embeddings.shape[0])\n",
        "                self.targets = self.targets[:min_length]\n",
        "                self.embeddings = self.embeddings[:min_length]\n",
        "                print(f\"Truncated to {min_length} matching examples\")\n",
        "        else:\n",
        "            self.targets = None\n",
        "            print(\"Warning: No targets created\")\n",
        "\n",
        "    def create_dataloader(self, batch_size=16, shuffle=True):\n",
        "        \"\"\"\n",
        "        Create a DataLoader for the dataset\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): Batch size\n",
        "            shuffle (bool): Whether to shuffle samples\n",
        "\n",
        "        Returns:\n",
        "            torch.utils.data.DataLoader: DataLoader for the dataset\n",
        "        \"\"\"\n",
        "        if self.targets is not None and self.embeddings is not None:\n",
        "            dataset = TensorDataset(self.embeddings, self.targets)\n",
        "\n",
        "            return DataLoader(\n",
        "                dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=shuffle,\n",
        "                num_workers=2,\n",
        "                pin_memory=True\n",
        "            )\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "class StockPredictionTrainer:\n",
        "    \"\"\"\n",
        "    Trainer for the stock prediction model\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, metrics_count=6, stocks_count=5, device='cuda'):\n",
        "        \"\"\"\n",
        "        Initialize the trainer\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): The model to train\n",
        "            optimizer: PyTorch optimizer\n",
        "            metrics_count (int): Number of metrics per stock (updated to 6)\n",
        "            stocks_count (int): Number of stocks\n",
        "            device (str): Device to use ('cuda' or 'cpu')\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.metrics_count = metrics_count\n",
        "        self.stocks_count = stocks_count\n",
        "        self.device = device\n",
        "        self.scheduler = None\n",
        "\n",
        "        # Use MSE loss, focusing specifically on close price\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Calculate the indices of closing prices in output\n",
        "        self.close_indices = []\n",
        "        for i in range(stocks_count):\n",
        "            # Close price is at index 3 in the metrics list\n",
        "            close_idx = i * metrics_count + 3\n",
        "            self.close_indices.append(close_idx)\n",
        "\n",
        "    def set_scheduler(self, scheduler):\n",
        "        \"\"\"Set learning rate scheduler\"\"\"\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def extract_close_prices(self, predictions, targets):\n",
        "        \"\"\"Extract only closing price predictions and targets\"\"\"\n",
        "        close_pred = predictions[:, self.close_indices]\n",
        "        close_targets = targets[:, self.close_indices]\n",
        "        return close_pred, close_targets\n",
        "\n",
        "    def train_step(self, inputs, targets):\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        # Move data to device\n",
        "        inputs = inputs.to(self.device)\n",
        "        targets = targets.to(self.device)\n",
        "\n",
        "        # Zero gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        # Extract closing prices for loss calculation\n",
        "        close_pred, close_targets = self.extract_close_prices(outputs, targets)\n",
        "\n",
        "        # Calculate loss on closing prices\n",
        "        loss = self.criterion(close_pred, close_targets)\n",
        "\n",
        "        # We also want to track the full MSE for all metrics\n",
        "        full_loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item(), full_loss.item()\n",
        "\n",
        "    def validate(self, val_dataloader):\n",
        "        \"\"\"Validate model on validation data\"\"\"\n",
        "        self.model.eval()\n",
        "        close_val_loss = 0\n",
        "        full_val_loss = 0\n",
        "        all_close_preds = []\n",
        "        all_close_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                inputs = batch[0].to(self.device)\n",
        "                targets = batch[1].to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "\n",
        "                # Extract closing prices\n",
        "                close_pred, close_targets = self.extract_close_prices(outputs, targets)\n",
        "\n",
        "                # Calculate losses\n",
        "                close_loss = self.criterion(close_pred, close_targets)\n",
        "                full_loss = self.criterion(outputs, targets)\n",
        "\n",
        "                close_val_loss += close_loss.item()\n",
        "                full_val_loss += full_loss.item()\n",
        "\n",
        "                all_close_preds.append(close_pred.cpu())\n",
        "                all_close_targets.append(close_targets.cpu())\n",
        "\n",
        "        # Stack predictions and targets\n",
        "        all_close_preds = torch.cat(all_close_preds, dim=0)\n",
        "        all_close_targets = torch.cat(all_close_targets, dim=0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        close_preds_np = all_close_preds.numpy()\n",
        "        close_targets_np = all_close_targets.numpy()\n",
        "\n",
        "        # Overall metrics for closing prices\n",
        "        close_mse = mean_squared_error(close_targets_np, close_preds_np)\n",
        "        close_r2 = r2_score(close_targets_np, close_preds_np)\n",
        "\n",
        "        # Per-stock metrics for closing prices\n",
        "        stock_metrics = {}\n",
        "        for s in range(self.stocks_count):\n",
        "            stock_pred = close_preds_np[:, s]\n",
        "            stock_target = close_targets_np[:, s]\n",
        "\n",
        "            stock_mse = mean_squared_error(stock_target, stock_pred)\n",
        "            stock_r2 = r2_score(stock_target, stock_pred)\n",
        "\n",
        "            stock_metrics[f'stock_{s}'] = {\n",
        "                'mse': stock_mse,\n",
        "                'r2': stock_r2\n",
        "            }\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        return {\n",
        "            'close_val_loss': close_val_loss / len(val_dataloader),\n",
        "            'full_val_loss': full_val_loss / len(val_dataloader),\n",
        "            'close_mse': close_mse,\n",
        "            'close_r2': close_r2,\n",
        "            'stock_metrics': stock_metrics\n",
        "        }\n",
        "\n",
        "    def predict(self, dataloader):\n",
        "        \"\"\"Generate predictions\"\"\"\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                inputs = batch[0].to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "                all_preds.append(outputs.cpu())\n",
        "\n",
        "        return torch.cat(all_preds, dim=0)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def train(self, train_dataloader, val_dataloader, epochs, save_path=None, early_stopping_patience=10):\n",
        "        \"\"\"\n",
        "        Train the model\n",
        "\n",
        "        Args:\n",
        "            train_dataloader: Training data loader\n",
        "            val_dataloader: Validation data loader\n",
        "            epochs (int): Number of epochs\n",
        "            save_path (str, optional): Path to save the best model\n",
        "            early_stopping_patience (int): Number of epochs to wait for improvement\n",
        "\n",
        "        Returns:\n",
        "            dict: Training history\n",
        "        \"\"\"\n",
        "        history = {\n",
        "            'train_close_loss': [],\n",
        "            'train_full_loss': [],\n",
        "            'val_close_loss': [],\n",
        "            'val_full_loss': [],\n",
        "            'close_mse': [],\n",
        "            'close_r2': []\n",
        "        }\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        no_improvement_count = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            train_close_loss = 0\n",
        "            train_full_loss = 0\n",
        "\n",
        "            for batch in train_dataloader:\n",
        "                inputs, targets = batch\n",
        "                close_loss, full_loss = self.train_step(inputs, targets)\n",
        "                train_close_loss += close_loss\n",
        "                train_full_loss += full_loss\n",
        "\n",
        "            avg_train_close_loss = train_close_loss / len(train_dataloader)\n",
        "            avg_train_full_loss = train_full_loss / len(train_dataloader)\n",
        "\n",
        "            # Validation\n",
        "            val_metrics = self.validate(val_dataloader)\n",
        "\n",
        "            # Update history\n",
        "            history['train_close_loss'].append(avg_train_close_loss)\n",
        "            history['train_full_loss'].append(avg_train_full_loss)\n",
        "            history['val_close_loss'].append(val_metrics['close_val_loss'])\n",
        "            history['val_full_loss'].append(val_metrics['full_val_loss'])\n",
        "            history['close_mse'].append(val_metrics['close_mse'])\n",
        "            history['close_r2'].append(val_metrics['close_r2'])\n",
        "\n",
        "            # Step scheduler if needed\n",
        "            if self.scheduler is not None:\n",
        "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    self.scheduler.step(val_metrics['close_val_loss'])\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "                  f\"Train Close Loss: {avg_train_close_loss:.6f}, \"\n",
        "                  f\"Val Close Loss: {val_metrics['close_val_loss']:.6f}, \"\n",
        "                  f\"Close MSE: {val_metrics['close_mse']:.6f}, \"\n",
        "                  f\"Close R²: {val_metrics['close_r2']:.6f}\")\n",
        "\n",
        "            # Check for early stopping\n",
        "            if val_metrics['close_val_loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['close_val_loss']\n",
        "                no_improvement_count = 0\n",
        "\n",
        "                # Save the best model\n",
        "                if save_path:\n",
        "                    self.save_model(save_path)\n",
        "            else:\n",
        "                no_improvement_count += 1\n",
        "                if no_improvement_count >= early_stopping_patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "def plot_metrics(history, save_dir=None):\n",
        "    \"\"\"\n",
        "    Plot training metrics\n",
        "\n",
        "    Args:\n",
        "        history (dict): Training history\n",
        "        save_dir (str, optional): Directory to save plots\n",
        "    \"\"\"\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Plot closing price loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_close_loss'], label='Train Close Loss')\n",
        "    plt.plot(history['val_close_loss'], label='Val Close Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss (Closing Price)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'close_loss_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot full loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_full_loss'], label='Train Full Loss')\n",
        "    plt.plot(history['val_full_loss'], label='Val Full Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss (All Metrics)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'full_loss_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot MSE for closing price\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['close_mse'], label='Validation Close MSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('Validation Mean Squared Error (Closing Price)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'close_mse_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot R² for closing price\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['close_r2'], label='Validation Close R²')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('R²')\n",
        "    plt.title('Validation R² Score (Closing Price)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'close_r2_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(\n",
        "    embeddings_dir,\n",
        "    stock_csv_path,\n",
        "    output_dir=\"output_tinybert_lora_stock_prediction\",\n",
        "    metrics_count=6,  # Updated to 6 features\n",
        "    stocks_count=5,\n",
        "    epochs=50,\n",
        "    batch_size=16,\n",
        "    lr=3e-5,\n",
        "    lora_r=8,\n",
        "    lora_alpha=16\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a TinyBERT with LoRA model for stock prediction\n",
        "\n",
        "    Args:\n",
        "        embeddings_dir (str): Directory containing embeddings and dates files\n",
        "        stock_csv_path (str): Path to stock prices CSV file\n",
        "        output_dir (str): Directory to save outputs\n",
        "        metrics_count (int): Number of metrics per stock (now 6)\n",
        "        stocks_count (int): Number of stocks\n",
        "        epochs (int): Number of training epochs\n",
        "        batch_size (int): Batch size\n",
        "        lr (float): Learning rate\n",
        "        lora_r (int): LoRA rank\n",
        "        lora_alpha (int): LoRA alpha\n",
        "    \"\"\"\n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Paths to embeddings and dates files\n",
        "    train_embeddings_path = os.path.join(embeddings_dir, \"train_embeddings_9x312.pt\")\n",
        "    train_dates_path = os.path.join(embeddings_dir, \"train_dates.csv\")\n",
        "\n",
        "    val_embeddings_path = os.path.join(embeddings_dir, \"val_embeddings_9x312.pt\")\n",
        "    val_dates_path = os.path.join(embeddings_dir, \"val_dates.csv\")\n",
        "\n",
        "    test_embeddings_path = os.path.join(embeddings_dir, \"test_embeddings_9x312.pt\")\n",
        "    test_dates_path = os.path.join(embeddings_dir, \"test_dates.csv\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = StockDataset(\n",
        "        train_embeddings_path,\n",
        "        train_dates_path,\n",
        "        stock_csv_path,\n",
        "        metrics_count,\n",
        "        stocks_count\n",
        "    )\n",
        "\n",
        "    val_dataset = StockDataset(\n",
        "        val_embeddings_path,\n",
        "        val_dates_path,\n",
        "        stock_csv_path,\n",
        "        metrics_count,\n",
        "        stocks_count\n",
        "    )\n",
        "\n",
        "    test_dataset = StockDataset(\n",
        "        test_embeddings_path,\n",
        "        test_dates_path,\n",
        "        stock_csv_path,\n",
        "        metrics_count,\n",
        "        stocks_count\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = train_dataset.create_dataloader(batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = val_dataset.create_dataloader(batch_size=batch_size, shuffle=False)\n",
        "    test_dataloader = test_dataset.create_dataloader(batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if not all([train_dataloader, val_dataloader, test_dataloader]):\n",
        "        print(\"Error: Failed to create one or more dataloaders. Check data matching.\")\n",
        "        return None\n",
        "\n",
        "    # Output dimension is metrics_count * stocks_count\n",
        "    output_dim = metrics_count * stocks_count\n",
        "\n",
        "    # Create model\n",
        "    model = TinyBERTStockPredictor(\n",
        "        input_dim=312,  # From the embeddings\n",
        "        hidden_dim=312,  # TinyBERT hidden size\n",
        "        output_dim=output_dim,\n",
        "        lora_r=lora_r,\n",
        "        lora_alpha=lora_alpha\n",
        "    )\n",
        "\n",
        "    print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Input dimension: 312, Output dimension: {output_dim}\")\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Create scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = StockPredictionTrainer(\n",
        "        model,\n",
        "        optimizer,\n",
        "        metrics_count=metrics_count,\n",
        "        stocks_count=stocks_count,\n",
        "        device=device\n",
        "    )\n",
        "    trainer.set_scheduler(scheduler)\n",
        "\n",
        "    # Train model\n",
        "    print(f\"\\nTraining for {epochs} epochs...\")\n",
        "    history = trainer.train(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        epochs=epochs,\n",
        "        save_path=os.path.join(output_dir, 'best_model.pt'),\n",
        "        early_stopping_patience=10\n",
        "    )\n",
        "\n",
        "    # Plot training metrics\n",
        "    plot_metrics(history, save_dir=output_dir)\n",
        "\n",
        "    # Generate test predictions\n",
        "    print(\"\\nGenerating test predictions...\")\n",
        "    test_predictions = trainer.predict(test_dataloader)\n",
        "\n",
        "    # Save test predictions\n",
        "    torch.save(test_predictions, os.path.join(output_dir, 'test_predictions.pt'))\n",
        "    print(f\"Test predictions saved to {os.path.join(output_dir, 'test_predictions.pt')}\")\n",
        "\n",
        "    # Extract closing prices from test predictions\n",
        "    close_indices = [i * metrics_count + 3 for i in range(stocks_count)]\n",
        "    close_predictions = test_predictions[:, close_indices]\n",
        "\n",
        "    # Save close predictions separately\n",
        "    torch.save(close_predictions, os.path.join(output_dir, 'test_close_predictions.pt'))\n",
        "    print(f\"Close price predictions saved to {os.path.join(output_dir, 'test_close_predictions.pt')}\")\n",
        "\n",
        "    # Final test evaluation\n",
        "    test_metrics = trainer.validate(test_dataloader)\n",
        "    print(\"\\nFinal test metrics:\")\n",
        "    print(f\"Close Loss: {test_metrics['close_val_loss']:.6f}\")\n",
        "    print(f\"Close MSE: {test_metrics['close_mse']:.6f}\")\n",
        "    print(f\"Close R²: {test_metrics['close_r2']:.6f}\")\n",
        "\n",
        "    # Per-stock metrics\n",
        "    print(\"\\nPer-stock closing price metrics:\")\n",
        "    for stock_idx, (stock_name, metrics) in enumerate(test_metrics['stock_metrics'].items()):\n",
        "        stock_symbol = ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX'][stock_idx % 5]\n",
        "        print(f\"{stock_symbol}: MSE={metrics['mse']:.6f}, R²={metrics['r2']:.6f}\")\n",
        "\n",
        "    # Save metrics as CSV\n",
        "    stock_metrics_df = pd.DataFrame({\n",
        "        'Stock': ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX'],\n",
        "        'MSE': [test_metrics['stock_metrics'][f'stock_{i}']['mse'] for i in range(stocks_count)],\n",
        "        'R2': [test_metrics['stock_metrics'][f'stock_{i}']['r2'] for i in range(stocks_count)]\n",
        "    })\n",
        "    stock_metrics_df.to_csv(os.path.join(output_dir, 'stock_metrics.csv'), index=False)\n",
        "\n",
        "    # Save overall metrics\n",
        "    overall_metrics = {\n",
        "        'close_mse': test_metrics['close_mse'],\n",
        "        'close_r2': test_metrics['close_r2'],\n",
        "        'close_loss': test_metrics['close_val_loss'],\n",
        "        'full_loss': test_metrics['full_val_loss']\n",
        "    }\n",
        "\n",
        "    pd.DataFrame([overall_metrics]).to_csv(os.path.join(output_dir, 'overall_metrics.csv'), index=False)\n",
        "\n",
        "    return test_metrics\n",
        "\n",
        "def remap_targets_to_model_format(targets, output_dim=30):\n",
        "    \"\"\"\n",
        "    Remaps the targets from the CSV format to the model's expected format.\n",
        "\n",
        "    Original format is grouped by metric then company:\n",
        "    [AAPL_AdjClose, AMZN_AdjClose, GOOGL_AdjClose, META_AdjClose, NFLX_AdjClose,\n",
        "     AAPL_Close, AMZN_Close, GOOGL_Close, META_Close, NFLX_Close, ...etc]\n",
        "\n",
        "    Model expects format grouped by company then metric:\n",
        "    [AAPL_Open, AAPL_High, AAPL_Low, AAPL_Close, AAPL_AdjClose, AAPL_Volume,\n",
        "     AMZN_Open, AMZN_High, ...etc]\n",
        "\n",
        "    Args:\n",
        "        targets (torch.Tensor): Tensor of shape [batch_size, 30] in CSV order\n",
        "        output_dim (int): Expected output dimension (default 30)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Reordered tensor in model's expected format\n",
        "    \"\"\"\n",
        "    batch_size = targets.shape[0]\n",
        "\n",
        "    # Create a new tensor for reordered targets\n",
        "    reordered = torch.zeros((batch_size, output_dim), dtype=targets.dtype)\n",
        "\n",
        "    # CSV order of metrics\n",
        "    csv_metrics = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "    # Model expected order of metrics\n",
        "    model_metrics = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
        "\n",
        "    # Number of stocks and metrics\n",
        "    stocks_count = 5\n",
        "    metrics_count = 6\n",
        "\n",
        "    # Do the reordering\n",
        "    for stock_idx in range(stocks_count):\n",
        "        for model_metric_idx, model_metric in enumerate(model_metrics):\n",
        "            # Find the corresponding index in the CSV format\n",
        "            csv_metric_idx = csv_metrics.index(model_metric)\n",
        "\n",
        "            # Calculate source and target indices\n",
        "            src_idx = csv_metric_idx * stocks_count + stock_idx\n",
        "            tgt_idx = stock_idx * metrics_count + model_metric_idx\n",
        "\n",
        "            # Copy the values\n",
        "            reordered[:, tgt_idx] = targets[:, src_idx]\n",
        "\n",
        "    return reordered\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the training and evaluation process\n",
        "    \"\"\"\n",
        "    # Define paths to required data\n",
        "    embeddings_dir = \"/content\"  # Base directory\n",
        "\n",
        "    # Embeddings files\n",
        "    train_embeddings_path = os.path.join(embeddings_dir, \"train_embeddings_9x312(3).pt\")\n",
        "    val_embeddings_path = os.path.join(embeddings_dir, \"val_embeddings_9x312(2).pt\")\n",
        "    test_embeddings_path = os.path.join(embeddings_dir, \"test_embeddings_9x312(1).pt\")\n",
        "\n",
        "    # Target CSV files - these contain the stock data for the 10th day\n",
        "    train_stocks_path = os.path.join(embeddings_dir, \"train.csv\")\n",
        "    val_stocks_path = os.path.join(embeddings_dir, \"val.csv\")\n",
        "    test_stocks_path = os.path.join(embeddings_dir, \"test.csv\")\n",
        "\n",
        "    # Output directory\n",
        "    output_dir = \"output_tinybert_lora_stock_prediction\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Ensure the directories exist\n",
        "    os.makedirs(embeddings_dir, exist_ok=True)\n",
        "\n",
        "    # Check if data files exist\n",
        "    required_files = [\n",
        "        train_embeddings_path,\n",
        "        val_embeddings_path,\n",
        "        test_embeddings_path,\n",
        "        train_stocks_path,\n",
        "        val_stocks_path,\n",
        "        test_stocks_path\n",
        "    ]\n",
        "\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "    if missing_files:\n",
        "        print(\"Error: The following required files are missing:\")\n",
        "        for file in missing_files:\n",
        "            print(f\" - {file}\")\n",
        "        print(\"\\nPlease ensure all required data files are available before running.\")\n",
        "        return\n",
        "\n",
        "    # Create datasets using the specific CSV format handler\n",
        "    train_dataset = StockDataset(\n",
        "        train_embeddings_path,\n",
        "        train_stocks_path,\n",
        "        metrics_count=6,\n",
        "        stocks_count=5\n",
        "    )\n",
        "\n",
        "    val_dataset = StockDataset(\n",
        "        val_embeddings_path,\n",
        "        val_stocks_path,\n",
        "        metrics_count=6,\n",
        "        stocks_count=5\n",
        "    )\n",
        "\n",
        "    test_dataset = StockDataset(\n",
        "        test_embeddings_path,\n",
        "        test_stocks_path,\n",
        "        metrics_count=6,\n",
        "        stocks_count=5\n",
        "    )\n",
        "\n",
        "    # Create dataloaders with a custom collate function to reorder targets\n",
        "    def collate_fn(batch):\n",
        "        inputs = torch.stack([item[0] for item in batch])\n",
        "        targets = torch.stack([item[1] for item in batch])\n",
        "        # Reorder targets to match model's expected format\n",
        "        reordered_targets = remap_targets_to_model_format(targets)\n",
        "        return inputs, reordered_targets\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        TensorDataset(train_dataset.embeddings, train_dataset.targets),\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        TensorDataset(val_dataset.embeddings, val_dataset.targets),\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        TensorDataset(test_dataset.embeddings, test_dataset.targets),\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    if not all([train_dataloader, val_dataloader, test_dataloader]):\n",
        "        print(\"Error: Failed to create one or more dataloaders. Check data matching.\")\n",
        "        return None\n",
        "\n",
        "    # Output dimension is metrics_count * stocks_count\n",
        "    output_dim = 6 * 5  # 6 metrics for 5 stocks\n",
        "\n",
        "    # Create model\n",
        "    model = TinyBERTStockPredictor(\n",
        "        input_dim=312,  # From the embeddings\n",
        "        hidden_dim=312,  # TinyBERT hidden size\n",
        "        output_dim=output_dim,\n",
        "        lora_r=8,\n",
        "        lora_alpha=16\n",
        "    )\n",
        "\n",
        "    print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Input dimension: 312, Output dimension: {output_dim}\")\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
        "\n",
        "    # Create scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = StockPredictionTrainer(\n",
        "        model,\n",
        "        optimizer,\n",
        "        metrics_count=6,\n",
        "        stocks_count=5,\n",
        "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    )\n",
        "    trainer.set_scheduler(scheduler)\n",
        "\n",
        "    # Train model\n",
        "    print(f\"\\nTraining for 50 epochs...\")\n",
        "    history = trainer.train(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        epochs=50,\n",
        "        save_path=os.path.join(output_dir, 'best_model.pt'),\n",
        "        early_stopping_patience=10\n",
        "    )\n",
        "\n",
        "    # Plot training metrics\n",
        "    plot_metrics(history, save_dir=output_dir)\n",
        "\n",
        "    # Generate test predictions\n",
        "    print(\"\\nGenerating test predictions...\")\n",
        "    test_predictions = trainer.predict(test_dataloader)\n",
        "\n",
        "    # Save test predictions\n",
        "    torch.save(test_predictions, os.path.join(output_dir, 'test_predictions.pt'))\n",
        "    print(f\"Test predictions saved to {os.path.join(output_dir, 'test_predictions.pt')}\")\n",
        "\n",
        "    # Extract closing prices from test predictions\n",
        "    close_indices = [i * 6 + 3 for i in range(5)]  # Index 3 is Close in the model's order\n",
        "    close_predictions = test_predictions[:, close_indices]\n",
        "\n",
        "    # Save close predictions separately\n",
        "    torch.save(close_predictions, os.path.join(output_dir, 'test_close_predictions.pt'))\n",
        "    print(f\"Close price predictions saved to {os.path.join(output_dir, 'test_close_predictions.pt')}\")\n",
        "\n",
        "    # Final test evaluation\n",
        "    test_metrics = trainer.validate(test_dataloader)\n",
        "    print(\"\\nFinal test metrics:\")\n",
        "    print(f\"Close Loss: {test_metrics['close_val_loss']:.6f}\")\n",
        "    print(f\"Close MSE: {test_metrics['close_mse']:.6f}\")\n",
        "    print(f\"Close R²: {test_metrics['close_r2']:.6f}\")\n",
        "\n",
        "    # Per-stock metrics\n",
        "    print(\"\\nPer-stock closing price metrics:\")\n",
        "    stock_symbols = ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX']\n",
        "    for stock_idx, (stock_name, metrics) in enumerate(test_metrics['stock_metrics'].items()):\n",
        "        stock_symbol = stock_symbols[stock_idx % 5]\n",
        "        print(f\"{stock_symbol}: MSE={metrics['mse']:.6f}, R²={metrics['r2']:.6f}\")\n",
        "\n",
        "    return test_metrics\n",
        "\n",
        "\n",
        "# Execute the main function when the script is run\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p80oF_frS555",
        "outputId": "8f47f02c-be8b-4441-8072-fb10f2437c84"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading embeddings from /content/train_embeddings_9x312(3).pt\n",
            "Loading stock data from /content/train.csv\n",
            "Loading stock price data from /content/train.csv...\n",
            "CSV structure sample:\n",
            "   Unnamed: 0  Adj Close  Adj Close.1  Adj Close.2  Adj Close.3  Adj Close.4  \\\n",
            "0  2018-01-09  40.966282    62.634998    55.374752   186.988708   209.309998   \n",
            "\n",
            "     Close    Close.1  Close.2     Close.3  ...     Open.1     Open.2  \\\n",
            "0  43.5825  62.634998  55.6395  187.869995  ...  62.845001  55.922001   \n",
            "\n",
            "       Open.3      Open.4      Volume    Volume.1    Volume.2    Volume.3  \\\n",
            "0  188.699997  212.110001  86336000.0  73226000.0  26808000.0  12393100.0   \n",
            "\n",
            "    Volume.4  is_business_day  \n",
            "0  6125900.0             True  \n",
            "\n",
            "[1 rows x 32 columns]\n",
            "CSV columns: ['Unnamed: 0', 'Adj Close', 'Adj Close.1', 'Adj Close.2', 'Adj Close.3', 'Adj Close.4', 'Close', 'Close.1', 'Close.2', 'Close.3', 'Close.4', 'High', 'High.1', 'High.2', 'High.3', 'High.4', 'Low', 'Low.1', 'Low.2', 'Low.3', 'Low.4', 'Open', 'Open.1', 'Open.2', 'Open.3', 'Open.4', 'Volume', 'Volume.1', 'Volume.2', 'Volume.3', 'Volume.4', 'is_business_day']\n",
            "Creating target tensors...\n",
            "Created 340 targets with shape torch.Size([340, 30])\n",
            "Warning: Number of targets (340) doesn't match number of embeddings (345)\n",
            "Truncated to 340 matching examples\n",
            "Loading embeddings from /content/val_embeddings_9x312(2).pt\n",
            "Loading stock data from /content/val.csv\n",
            "Loading stock price data from /content/val.csv...\n",
            "CSV structure sample:\n",
            "   Unnamed: 0  Adj Close  Adj Close.1  Adj Close.2  Adj Close.3  Adj Close.4  \\\n",
            "0  2018-03-13  42.464058    79.408997      56.7243   181.026825   315.880005   \n",
            "\n",
            "     Close    Close.1    Close.2     Close.3  ...     Open.1     Open.2  \\\n",
            "0  44.9925  79.408997  56.995499  181.880005  ...  80.797997  58.591499   \n",
            "\n",
            "       Open.3      Open.4       Volume     Volume.1    Volume.2    Volume.3  \\\n",
            "0  185.610001  323.869995  126774000.0  130638000.0  43140000.0  18067500.0   \n",
            "\n",
            "     Volume.4  is_business_day  \n",
            "0  12917200.0             True  \n",
            "\n",
            "[1 rows x 32 columns]\n",
            "CSV columns: ['Unnamed: 0', 'Adj Close', 'Adj Close.1', 'Adj Close.2', 'Adj Close.3', 'Adj Close.4', 'Close', 'Close.1', 'Close.2', 'Close.3', 'Close.4', 'High', 'High.1', 'High.2', 'High.3', 'High.4', 'Low', 'Low.1', 'Low.2', 'Low.3', 'Low.4', 'Open', 'Open.1', 'Open.2', 'Open.3', 'Open.4', 'Volume', 'Volume.1', 'Volume.2', 'Volume.3', 'Volume.4', 'is_business_day']\n",
            "Creating target tensors...\n",
            "Created 44 targets with shape torch.Size([44, 30])\n",
            "Loading embeddings from /content/test_embeddings_9x312(1).pt\n",
            "Loading stock data from /content/test.csv\n",
            "Loading stock price data from /content/test.csv...\n",
            "CSV structure sample:\n",
            "   Unnamed: 0  Adj Close  Adj Close.1  Adj Close.2  Adj Close.3  Adj Close.4  \\\n",
            "0  2018-03-19  41.362183    77.246498    54.741783   171.750534   313.480011   \n",
            "\n",
            "       Close    Close.1    Close.2     Close.3  ...     Open.1  Open.2  \\\n",
            "0  43.825001  77.246498  55.003502  172.559998  ...  77.726501  55.888   \n",
            "\n",
            "       Open.3      Open.4       Volume     Volume.1    Volume.2    Volume.3  \\\n",
            "0  177.009995  315.799988  133787200.0  131616000.0  63656000.0  88140100.0   \n",
            "\n",
            "    Volume.4  is_business_day  \n",
            "0  9925200.0             True  \n",
            "\n",
            "[1 rows x 32 columns]\n",
            "CSV columns: ['Unnamed: 0', 'Adj Close', 'Adj Close.1', 'Adj Close.2', 'Adj Close.3', 'Adj Close.4', 'Close', 'Close.1', 'Close.2', 'Close.3', 'Close.4', 'High', 'High.1', 'High.2', 'High.3', 'High.4', 'Low', 'Low.1', 'Low.2', 'Low.3', 'Low.4', 'Open', 'Open.1', 'Open.2', 'Open.3', 'Open.4', 'Volume', 'Volume.1', 'Volume.2', 'Volume.3', 'Volume.4', 'is_business_day']\n",
            "Creating target tensors...\n",
            "Created 42 targets with shape torch.Size([42, 30])\n",
            "Warning: Number of targets (42) doesn't match number of embeddings (43)\n",
            "Truncated to 42 matching examples\n",
            "Model created with 14507078 parameters\n",
            "Input dimension: 312, Output dimension: 30\n",
            "\n",
            "Training for 50 epochs...\n",
            "Epoch 1/50 - Train Close Loss: 4722537714465327.000000, Val Close Loss: 4562353348324011.000000, Close MSE: 4682000757686272.000000, Close R²: -160.732697\n",
            "Model saved to output_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 2/50 - Train Close Loss: 4762267790868480.000000, Val Close Loss: 4562353348324011.000000, Close MSE: 4682000757686272.000000, Close R²: -160.788849\n",
            "Epoch 3/50 - Train Close Loss: 4752987330469143.000000, Val Close Loss: 4562353348324011.000000, Close MSE: 4682000757686272.000000, Close R²: -160.791916\n",
            "Epoch 4/50 - Train Close Loss: 4883352485006243.000000, Val Close Loss: 4562353348324011.000000, Close MSE: 4682000757686272.000000, Close R²: -160.680679\n",
            "Epoch 5/50 - Train Close Loss: 4842562924362287.000000, Val Close Loss: 4562353348324011.000000, Close MSE: 4682000757686272.000000, Close R²: -160.318069\n",
            "Epoch 6/50 - Train Close Loss: 4721022298703313.000000, Val Close Loss: 4562353258845525.000000, Close MSE: 4682000757686272.000000, Close R²: -159.488449\n",
            "Model saved to output_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 7/50 - Train Close Loss: 4807000216901446.000000, Val Close Loss: 4562353258845525.000000, Close MSE: 4682000757686272.000000, Close R²: -158.019318\n",
            "Epoch 8/50 - Train Close Loss: 4772157355720704.000000, Val Close Loss: 4562352900931584.000000, Close MSE: 4681999683944448.000000, Close R²: -157.024872\n",
            "Model saved to output_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 9/50 - Train Close Loss: 4819619294478336.000000, Val Close Loss: 4562352900931584.000000, Close MSE: 4681999683944448.000000, Close R²: -155.896393\n",
            "Epoch 10/50 - Train Close Loss: 4754042550246679.000000, Val Close Loss: 4562352721974613.000000, Close MSE: 4681999683944448.000000, Close R²: -154.634445\n",
            "Model saved to output_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 11/50 - Train Close Loss: 4758800922551017.000000, Val Close Loss: 4562352721974613.000000, Close MSE: 4681999683944448.000000, Close R²: -153.216614\n",
            "Epoch 12/50 - Train Close Loss: 4965199163234118.000000, Val Close Loss: 4562352721974613.000000, Close MSE: 4681999683944448.000000, Close R²: -151.652344\n",
            "Epoch 13/50 - Train Close Loss: 4759141493935011.000000, Val Close Loss: 4562352364060672.000000, Close MSE: 4681999147073536.000000, Close R²: -149.907837\n",
            "Model saved to output_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 14/50 - Train Close Loss: 4887917449564532.000000, Val Close Loss: 4562352364060672.000000, Close MSE: 4681999147073536.000000, Close R²: -148.988571\n",
            "Epoch 15/50 - Train Close Loss: 4837034862194316.000000, Val Close Loss: 4562352364060672.000000, Close MSE: 4681999147073536.000000, Close R²: -148.043549\n",
            "Epoch 16/50 - Train Close Loss: 4944182851117801.000000, Val Close Loss: 4562352185103701.000000, Close MSE: 4681998610202624.000000, Close R²: -147.077606\n",
            "Model saved to output_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 17/50 - Train Close Loss: 4807564468229958.000000, Val Close Loss: 4562352185103701.000000, Close MSE: 4681998610202624.000000, Close R²: -146.098663\n",
            "Epoch 18/50 - Train Close Loss: 4921595727922641.000000, Val Close Loss: 4562352185103701.000000, Close MSE: 4681998610202624.000000, Close R²: -145.102142\n",
            "Epoch 19/50 - Train Close Loss: 4845640451454045.000000, Val Close Loss: 4562352185103701.000000, Close MSE: 4681998610202624.000000, Close R²: -144.070786\n",
            "Epoch 20/50 - Train Close Loss: 4788433963998487.000000, Val Close Loss: 4562352185103701.000000, Close MSE: 4681998610202624.000000, Close R²: -143.549271\n",
            "Epoch 21/50 - Train Close Loss: 4821570185759651.000000, Val Close Loss: 4562352185103701.000000, Close MSE: 4681998610202624.000000, Close R²: -143.025436\n",
            "Epoch 22/50 - Train Close Loss: 4750728092260166.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -142.505478\n",
            "Model saved to output_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 23/50 - Train Close Loss: 4786292105293452.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -141.990662\n",
            "Epoch 24/50 - Train Close Loss: 4880458006492253.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -141.466965\n",
            "Epoch 25/50 - Train Close Loss: 4972639571792244.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -140.938873\n",
            "Epoch 26/50 - Train Close Loss: 4989287084660550.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -140.672089\n",
            "Epoch 27/50 - Train Close Loss: 4867465669413795.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -140.408432\n",
            "Epoch 28/50 - Train Close Loss: 4782731040534156.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -140.146027\n",
            "Epoch 29/50 - Train Close Loss: 4760264737697420.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -139.886826\n",
            "Epoch 30/50 - Train Close Loss: 4844439056369105.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -139.626251\n",
            "Epoch 31/50 - Train Close Loss: 4807031257801449.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -139.367981\n",
            "Epoch 32/50 - Train Close Loss: 4774779347843631.000000, Val Close Loss: 4562351827189760.000000, Close MSE: 4681998610202624.000000, Close R²: -139.239105\n",
            "Early stopping at epoch 32\n",
            "\n",
            "Generating test predictions...\n",
            "Test predictions saved to output_tinybert_lora_stock_prediction/test_predictions.pt\n",
            "Close price predictions saved to output_tinybert_lora_stock_prediction/test_close_predictions.pt\n",
            "\n",
            "Final test metrics:\n",
            "Close Loss: 5846509557208405.000000\n",
            "Close MSE: 5830332741844992.000000\n",
            "Close R²: -232.790924\n",
            "\n",
            "Per-stock closing price metrics:\n",
            "AAPL: MSE=29151663172354048.000000, R²=-8.337712\n",
            "AMZN: MSE=5118.715332, R²=-313.025787\n",
            "GOOGL: MSE=4682.500977, R²=-286.264648\n",
            "META: MSE=4554.543457, R²=-302.574280\n",
            "NFLX: MSE=4731.329590, R²=-253.752167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class NormalizationLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom normalization layer for financial data\n",
        "    \"\"\"\n",
        "    def __init__(self, method='standard'):\n",
        "        super().__init__()\n",
        "        self.method = method\n",
        "        if method == 'standard':\n",
        "            self.normalizer = StandardScaler()\n",
        "        elif method == 'minmax':\n",
        "            self.normalizer = MinMaxScaler(feature_range=(-1, 1))\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, features]\n",
        "        batch_size, seq_len, features = x.shape\n",
        "\n",
        "        # Reshape for sklearn normalizer\n",
        "        x_reshaped = x.reshape(-1, features)\n",
        "\n",
        "        if not self.is_fitted:\n",
        "            x_normalized = torch.tensor(\n",
        "                self.normalizer.fit_transform(x_reshaped.detach().cpu().numpy()),\n",
        "                dtype=x.dtype, device=x.device\n",
        "            )\n",
        "            self.is_fitted = True\n",
        "        else:\n",
        "            x_normalized = torch.tensor(\n",
        "                self.normalizer.transform(x_reshaped.detach().cpu().numpy()),\n",
        "                dtype=x.dtype, device=x.device\n",
        "            )\n",
        "\n",
        "        # Reshape back\n",
        "        return x_normalized.reshape(batch_size, seq_len, features)\n",
        "\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Linear layer with Low-Rank Adaptation (LoRA)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, r=8, alpha=16):\n",
        "        super().__init__()\n",
        "        self.original = nn.Linear(in_features, out_features)\n",
        "        self.lora_A = nn.Parameter(torch.zeros(in_features, r))\n",
        "        self.lora_B = nn.Parameter(torch.zeros(r, out_features))\n",
        "        self.scaling = alpha / r\n",
        "        self.r = r\n",
        "        # Initialize weights for LoRA\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Original path\n",
        "        original_output = self.original(x)\n",
        "        # LoRA path\n",
        "        lora_output = (x @ self.lora_A) @ self.lora_B\n",
        "        # Combine with scaling\n",
        "        return original_output + (lora_output * self.scaling)\n",
        "\n",
        "\n",
        "class LoRATransformerWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrap a transformer model with LoRA adaptation in attention layers\n",
        "    \"\"\"\n",
        "    def __init__(self, model, r=8, alpha=16):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.apply_lora()\n",
        "\n",
        "    def apply_lora(self):\n",
        "        \"\"\"\n",
        "        Apply LoRA to the query and value projection layers in attention blocks\n",
        "        \"\"\"\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.Linear) and any(key in name for key in ['query', 'value']):\n",
        "                in_features, out_features = module.in_features, module.out_features\n",
        "                parent_name = '.'.join(name.split('.')[:-1])\n",
        "                layer_name = name.split('.')[-1]\n",
        "\n",
        "                # Create LoRA layer\n",
        "                lora_layer = LoRALinear(in_features, out_features, r=self.r, alpha=self.alpha)\n",
        "                # Copy weights from original layer\n",
        "                lora_layer.original.weight.data = module.weight.data.clone()\n",
        "                if module.bias is not None:\n",
        "                    lora_layer.original.bias.data = module.bias.data.clone()\n",
        "\n",
        "                # Set the LoRA layer in the parent module\n",
        "                parent = self.model\n",
        "                for part in parent_name.split('.'):\n",
        "                    if part:\n",
        "                        parent = getattr(parent, part)\n",
        "                setattr(parent, layer_name, lora_layer)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.model(*args, **kwargs)\n",
        "\n",
        "\n",
        "class ImprovedTinyBERTStockPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved stock prediction model with normalization and residual connections\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=312, hidden_dim=312, output_dim=30, lora_r=8, lora_alpha=16):\n",
        "        super().__init__()\n",
        "\n",
        "        # Add normalization layer\n",
        "        self.norm_layer = NormalizationLayer(method='minmax')\n",
        "\n",
        "        # Load TinyBERT model\n",
        "        self.bert_config = AutoConfig.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
        "        self.bert = AutoModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
        "\n",
        "        # Apply LoRA to TinyBERT\n",
        "        self.bert = LoRATransformerWrapper(self.bert, r=lora_r, alpha=lora_alpha)\n",
        "\n",
        "        # Output projection layers with residual connections and batch normalization\n",
        "        bert_output_dim = self.bert_config.hidden_size  # 312\n",
        "\n",
        "        # First block\n",
        "        self.fc1 = nn.Linear(bert_output_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "\n",
        "        # Second block\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(128, output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass with normalization and residual connections\"\"\"\n",
        "        batch_size, seq_len, embed_dim = x.shape  # batch, 9, 312\n",
        "\n",
        "        # Normalize inputs\n",
        "        x = self.norm_layer(x)\n",
        "\n",
        "        # Create attention mask (all 1s as we want to attend to all tokens)\n",
        "        attention_mask = torch.ones(batch_size, seq_len, device=x.device)\n",
        "\n",
        "        # Pass through TinyBERT\n",
        "        bert_output = self.bert(inputs_embeds=x, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.pooler_output  # [batch, 312]\n",
        "\n",
        "        # First block with residual connection\n",
        "        x1 = self.fc1(pooled_output)\n",
        "        x1 = self.bn1(x1)\n",
        "        x1 = F.relu(x1)\n",
        "        x1 = self.dropout(x1)\n",
        "\n",
        "        # Second block with residual connection\n",
        "        x2 = self.fc2(x1)\n",
        "        x2 = self.bn2(x2)\n",
        "        x2 = F.relu(x2)\n",
        "        x2 = self.dropout(x2)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.fc_out(x2)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class StockDataset:\n",
        "    \"\"\"\n",
        "    Dataset for loading pre-processed embeddings and matching them with target values\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings_path, stock_csv_path, metrics_count=6, stocks_count=5):\n",
        "        \"\"\"\n",
        "        Initialize with paths to embeddings and stock data\n",
        "\n",
        "        Args:\n",
        "            embeddings_path (str): Path to embeddings tensor file (.pt)\n",
        "            stock_csv_path (str): Path to CSV with target stock values for 10th day\n",
        "            metrics_count (int): Number of metrics per stock (OHLCV + Adj Close = 6)\n",
        "            stocks_count (int): Number of stocks\n",
        "        \"\"\"\n",
        "        self.embeddings_path = embeddings_path\n",
        "        self.stock_csv_path = stock_csv_path\n",
        "        self.metrics_count = metrics_count\n",
        "        self.stocks_count = stocks_count\n",
        "\n",
        "        # Stock order mapping\n",
        "        self.stock_order = ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX']\n",
        "\n",
        "        # Load the data\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Load embeddings and stock data, then create target tensors\n",
        "        \"\"\"\n",
        "        print(f\"Loading embeddings from {self.embeddings_path}\")\n",
        "        self.embeddings = torch.load(self.embeddings_path)\n",
        "\n",
        "        print(f\"Loading stock data from {self.stock_csv_path}\")\n",
        "        self.stock_df = self.load_stock_csv()\n",
        "\n",
        "        # Create target tensors from stock CSV\n",
        "        self.create_targets()\n",
        "\n",
        "    def load_stock_csv(self):\n",
        "        \"\"\"Load and preprocess stock CSV file with target values\"\"\"\n",
        "        print(f\"Loading stock price data from {self.stock_csv_path}...\")\n",
        "\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(self.stock_csv_path)\n",
        "\n",
        "        print(\"CSV structure sample:\")\n",
        "        print(df.head(1))\n",
        "        print(f\"CSV columns: {df.columns.tolist()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_targets(self):\n",
        "        \"\"\"\n",
        "        Create target tensors based on the specific CSV format with numbered columns\n",
        "        Format: [Metric], [Metric].1, [Metric].2, etc. for each company\n",
        "        \"\"\"\n",
        "        print(\"Creating target tensors...\")\n",
        "\n",
        "        # Base metrics in your CSV\n",
        "        base_metrics = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "        # List to store all targets\n",
        "        all_targets = []\n",
        "\n",
        "        # Process each row in the dataframe\n",
        "        for idx, row in self.stock_df.iterrows():\n",
        "            # Extract target values for this row\n",
        "            row_targets = []\n",
        "\n",
        "            # For each stock (in order)\n",
        "            for stock_idx in range(self.stocks_count):\n",
        "                # For each metric\n",
        "                for metric in base_metrics:\n",
        "                    # For the first stock, column name is just the metric\n",
        "                    # For subsequent stocks, column name is metric.1, metric.2, etc.\n",
        "                    if stock_idx == 0:\n",
        "                        col_name = metric\n",
        "                    else:\n",
        "                        col_name = f\"{metric}.{stock_idx}\"\n",
        "\n",
        "                    # Get the value, with error handling\n",
        "                    try:\n",
        "                        value = float(row[col_name])\n",
        "                        row_targets.append(value)\n",
        "                    except (KeyError, ValueError) as e:\n",
        "                        print(f\"Error extracting {col_name} (for {self.stock_order[stock_idx]}): {e}\")\n",
        "                        # Use 0.0 as a fallback value\n",
        "                        row_targets.append(0.0)\n",
        "\n",
        "            all_targets.append(row_targets)\n",
        "\n",
        "        if all_targets:\n",
        "            self.targets = torch.tensor(all_targets, dtype=torch.float32)\n",
        "            print(f\"Created {len(all_targets)} targets with shape {self.targets.shape}\")\n",
        "\n",
        "            # Make sure we have the same number of targets as embeddings\n",
        "            if len(all_targets) != self.embeddings.shape[0]:\n",
        "                print(f\"Warning: Number of targets ({len(all_targets)}) doesn't match number of embeddings ({self.embeddings.shape[0]})\")\n",
        "\n",
        "                # Take the minimum number to ensure alignment\n",
        "                min_length = min(len(all_targets), self.embeddings.shape[0])\n",
        "                self.targets = self.targets[:min_length]\n",
        "                self.embeddings = self.embeddings[:min_length]\n",
        "                print(f\"Truncated to {min_length} matching examples\")\n",
        "        else:\n",
        "            self.targets = None\n",
        "            print(\"Warning: No targets created\")\n",
        "\n",
        "    def create_dataloader(self, batch_size=16, shuffle=True):\n",
        "        \"\"\"\n",
        "        Create a DataLoader for the dataset\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): Batch size\n",
        "            shuffle (bool): Whether to shuffle samples\n",
        "\n",
        "        Returns:\n",
        "            torch.utils.data.DataLoader: DataLoader for the dataset\n",
        "        \"\"\"\n",
        "        if self.targets is not None and self.embeddings is not None:\n",
        "            dataset = TensorDataset(self.embeddings, self.targets)\n",
        "\n",
        "            return DataLoader(\n",
        "                dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=shuffle,\n",
        "                num_workers=2,\n",
        "                pin_memory=True\n",
        "            )\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "class ImprovedStockPredictionTrainer:\n",
        "    \"\"\"\n",
        "    Improved trainer for the stock prediction model\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, metrics_count=6, stocks_count=5, device='cuda'):\n",
        "        \"\"\"\n",
        "        Initialize the trainer with improved metrics\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.metrics_count = metrics_count\n",
        "        self.stocks_count = stocks_count\n",
        "        self.device = device\n",
        "        self.scheduler = None\n",
        "\n",
        "        # Use Huber loss for robustness against outliers\n",
        "        self.criterion = nn.HuberLoss(delta=1.0)\n",
        "\n",
        "        # Calculate the indices of closing prices in output\n",
        "        self.close_indices = []\n",
        "        for i in range(stocks_count):\n",
        "            # Close price is index 3 in the metrics list\n",
        "            close_idx = i * metrics_count + 3\n",
        "            self.close_indices.append(close_idx)\n",
        "\n",
        "        # Initialize past targets and predictions for directional accuracy\n",
        "        self.prev_close_targets = None\n",
        "        self.prev_close_preds = None\n",
        "\n",
        "    def set_scheduler(self, scheduler):\n",
        "        \"\"\"Set learning rate scheduler\"\"\"\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def extract_close_prices(self, predictions, targets):\n",
        "        \"\"\"Extract only closing price predictions and targets\"\"\"\n",
        "        close_pred = predictions[:, self.close_indices]\n",
        "        close_targets = targets[:, self.close_indices]\n",
        "        return close_pred, close_targets\n",
        "\n",
        "    def calculate_directional_accuracy(self, current_preds, current_targets, prev_preds=None, prev_targets=None):\n",
        "        \"\"\"\n",
        "        Calculate directional accuracy (up/down prediction accuracy)\n",
        "        \"\"\"\n",
        "        # If no previous data provided, use stored values\n",
        "        if prev_preds is None:\n",
        "            prev_preds = self.prev_close_preds\n",
        "        if prev_targets is None:\n",
        "            prev_targets = self.prev_close_targets\n",
        "\n",
        "        # If we still don't have previous values, we can't calculate direction\n",
        "        if prev_preds is None or prev_targets is None:\n",
        "            # Store current values for next time\n",
        "            self.prev_close_preds = current_preds.detach().clone()\n",
        "            self.prev_close_targets = current_targets.detach().clone()\n",
        "            return None\n",
        "\n",
        "        # Ensure we compare only the overlapping batch size\n",
        "        min_batch_size = min(current_preds.shape[0], prev_preds.shape[0])\n",
        "\n",
        "        # Convert to numpy and use only the overlapping batch size\n",
        "        curr_preds_np = current_preds[:min_batch_size].detach().cpu().numpy()\n",
        "        curr_targets_np = current_targets[:min_batch_size].detach().cpu().numpy()\n",
        "        prev_preds_np = prev_preds[:min_batch_size].detach().cpu().numpy()\n",
        "        prev_targets_np = prev_targets[:min_batch_size].detach().cpu().numpy()\n",
        "\n",
        "        # Calculate directions (1 for up, 0 for down or same)\n",
        "        pred_direction = (curr_preds_np > prev_preds_np).astype(int)\n",
        "        target_direction = (curr_targets_np > prev_targets_np).astype(int)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct = (pred_direction == target_direction)\n",
        "        accuracy = np.mean(correct)\n",
        "\n",
        "        # Store current values for next time (keep full batch)\n",
        "        self.prev_close_preds = current_preds.detach().clone()\n",
        "        self.prev_close_targets = current_targets.detach().clone()\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def train_step(self, inputs, targets):\n",
        "        \"\"\"Single training step with gradient clipping\"\"\"\n",
        "        # Move data to device\n",
        "        inputs = inputs.to(self.device)\n",
        "        targets = targets.to(self.device)\n",
        "\n",
        "        # Zero gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        # Extract closing prices for loss calculation\n",
        "        close_pred, close_targets = self.extract_close_prices(outputs, targets)\n",
        "\n",
        "        # Calculate loss on closing prices\n",
        "        loss = self.criterion(close_pred, close_targets)\n",
        "\n",
        "        # We also want to track the full loss for all metrics\n",
        "        full_loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Calculate directional accuracy\n",
        "        dir_accuracy = self.calculate_directional_accuracy(close_pred, close_targets)\n",
        "\n",
        "        return loss.item(), full_loss.item(), dir_accuracy\n",
        "\n",
        "    def validate(self, val_dataloader):\n",
        "        \"\"\"Validate model on validation data with direction accuracy\"\"\"\n",
        "        self.model.eval()\n",
        "        close_val_loss = 0\n",
        "        full_val_loss = 0\n",
        "        all_close_preds = []\n",
        "        all_close_targets = []\n",
        "        directional_accuracies = []\n",
        "\n",
        "        # Reset previous values for validation\n",
        "        prev_close_preds = None\n",
        "        prev_close_targets = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                inputs = batch[0].to(self.device)\n",
        "                targets = batch[1].to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "\n",
        "                # Extract closing prices\n",
        "                close_pred, close_targets = self.extract_close_prices(outputs, targets)\n",
        "\n",
        "                # Calculate losses\n",
        "                close_loss = self.criterion(close_pred, close_targets)\n",
        "                full_loss = self.criterion(outputs, targets)\n",
        "\n",
        "                close_val_loss += close_loss.item()\n",
        "                full_val_loss += full_loss.item()\n",
        "\n",
        "                # Calculate directional accuracy if we have previous values\n",
        "                if prev_close_preds is not None and prev_close_targets is not None:\n",
        "                    dir_acc = self.calculate_directional_accuracy(\n",
        "                        close_pred, close_targets, prev_close_preds, prev_close_targets\n",
        "                    )\n",
        "                    if dir_acc is not None:\n",
        "                        directional_accuracies.append(dir_acc)\n",
        "\n",
        "                # Store for next iteration\n",
        "                prev_close_preds = close_pred.detach().clone()\n",
        "                prev_close_targets = close_targets.detach().clone()\n",
        "\n",
        "                all_close_preds.append(close_pred.cpu())\n",
        "                all_close_targets.append(close_targets.cpu())\n",
        "\n",
        "        # Stack predictions and targets\n",
        "        all_close_preds = torch.cat(all_close_preds, dim=0)\n",
        "        all_close_targets = torch.cat(all_close_targets, dim=0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        close_preds_np = all_close_preds.numpy()\n",
        "        close_targets_np = all_close_targets.numpy()\n",
        "\n",
        "        # Overall metrics for closing prices (scale down to avoid overflow)\n",
        "        scale_factor = 1.0\n",
        "        if np.max(np.abs(close_targets_np)) > 1000:\n",
        "            scale_factor = np.max(np.abs(close_targets_np))\n",
        "\n",
        "        scaled_preds = close_preds_np / scale_factor\n",
        "        scaled_targets = close_targets_np / scale_factor\n",
        "\n",
        "        close_mse = mean_squared_error(scaled_targets, scaled_preds)\n",
        "\n",
        "        # Handle cases where R2 might overflow or be unstable\n",
        "        try:\n",
        "            close_r2 = r2_score(scaled_targets, scaled_preds)\n",
        "        except:\n",
        "            close_r2 = float('nan')\n",
        "\n",
        "        # Average directional accuracy\n",
        "        dir_accuracy = np.mean(directional_accuracies) if directional_accuracies else float('nan')\n",
        "\n",
        "        # Per-stock metrics for closing prices\n",
        "        stock_metrics = {}\n",
        "        for s in range(self.stocks_count):\n",
        "            stock_pred = scaled_preds[:, s]\n",
        "            stock_target = scaled_targets[:, s]\n",
        "\n",
        "            stock_mse = mean_squared_error(stock_target, stock_pred)\n",
        "\n",
        "            try:\n",
        "                stock_r2 = r2_score(stock_target, stock_pred)\n",
        "            except:\n",
        "                stock_r2 = float('nan')\n",
        "\n",
        "            # Calculate directional accuracy per stock\n",
        "            stock_dir_acc = self.calculate_stock_directional_accuracy(\n",
        "                all_close_preds[:, s].numpy(),\n",
        "                all_close_targets[:, s].numpy()\n",
        "            )\n",
        "\n",
        "            stock_metrics[f'stock_{s}'] = {\n",
        "                'mse': stock_mse,\n",
        "                'r2': stock_r2,\n",
        "                'dir_accuracy': stock_dir_acc\n",
        "            }\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        return {\n",
        "            'close_val_loss': close_val_loss / len(val_dataloader),\n",
        "            'full_val_loss': full_val_loss / len(val_dataloader),\n",
        "            'close_mse': close_mse,\n",
        "            'close_r2': close_r2,\n",
        "            'directional_accuracy': dir_accuracy,\n",
        "            'stock_metrics': stock_metrics\n",
        "        }\n",
        "\n",
        "    def calculate_stock_directional_accuracy(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Calculate directional accuracy for a single stock's time series\n",
        "        \"\"\"\n",
        "        if len(predictions) <= 1 or len(targets) <= 1:\n",
        "            return float('nan')\n",
        "\n",
        "        # Calculate day-to-day changes\n",
        "        pred_changes = np.diff(predictions)\n",
        "        target_changes = np.diff(targets)\n",
        "\n",
        "        # Convert to directional signals (1 for up, 0 for down/same)\n",
        "        pred_direction = (pred_changes > 0).astype(int)\n",
        "        target_direction = (target_changes > 0).astype(int)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct = (pred_direction == target_direction)\n",
        "        accuracy = np.mean(correct)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    # Other methods remain the same\n",
        "    # (predict, save_model, train)\n",
        "    def predict(self, dataloader):\n",
        "        \"\"\"Generate predictions\"\"\"\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                inputs = batch[0].to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "                all_preds.append(outputs.cpu())\n",
        "\n",
        "        return torch.cat(all_preds, dim=0)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def train(self, train_dataloader, val_dataloader, epochs, save_path=None, early_stopping_patience=10):\n",
        "        \"\"\"\n",
        "        Train the model with improved tracking and early stopping\n",
        "        \"\"\"\n",
        "        history = {\n",
        "            'train_close_loss': [],\n",
        "            'train_full_loss': [],\n",
        "            'val_close_loss': [],\n",
        "            'val_full_loss': [],\n",
        "            'close_mse': [],\n",
        "            'close_r2': [],\n",
        "            'directional_accuracy': []\n",
        "        }\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        no_improvement_count = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            train_close_loss = 0\n",
        "            train_full_loss = 0\n",
        "            train_dir_accuracies = []\n",
        "\n",
        "            for batch in train_dataloader:\n",
        "                inputs, targets = batch\n",
        "                close_loss, full_loss, dir_acc = self.train_step(inputs, targets)\n",
        "                train_close_loss += close_loss\n",
        "                train_full_loss += full_loss\n",
        "                if dir_acc is not None:\n",
        "                    train_dir_accuracies.append(dir_acc)\n",
        "\n",
        "            avg_train_close_loss = train_close_loss / len(train_dataloader)\n",
        "            avg_train_full_loss = train_full_loss / len(train_dataloader)\n",
        "            avg_train_dir_acc = np.mean(train_dir_accuracies) if train_dir_accuracies else float('nan')\n",
        "\n",
        "            # Validation\n",
        "            val_metrics = self.validate(val_dataloader)\n",
        "\n",
        "            # Update history\n",
        "            history['train_close_loss'].append(avg_train_close_loss)\n",
        "            history['train_full_loss'].append(avg_train_full_loss)\n",
        "            history['val_close_loss'].append(val_metrics['close_val_loss'])\n",
        "            history['val_full_loss'].append(val_metrics['full_val_loss'])\n",
        "            history['close_mse'].append(val_metrics['close_mse'])\n",
        "            history['close_r2'].append(val_metrics['close_r2'])\n",
        "            history['directional_accuracy'].append(val_metrics.get('directional_accuracy', float('nan')))\n",
        "\n",
        "            # Step scheduler if needed\n",
        "            if self.scheduler is not None:\n",
        "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    self.scheduler.step(val_metrics['close_val_loss'])\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "                  f\"Train Close Loss: {avg_train_close_loss:.6f}, \"\n",
        "                  f\"Val Close Loss: {val_metrics['close_val_loss']:.6f}, \"\n",
        "                  f\"Close MSE: {val_metrics['close_mse']:.6f}, \"\n",
        "                  f\"Close R²: {val_metrics['close_r2']:.6f}, \"\n",
        "                  f\"Dir Acc: {val_metrics.get('directional_accuracy', float('nan')):.4f}\")\n",
        "\n",
        "            # Check for early stopping\n",
        "            if val_metrics['close_val_loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['close_val_loss']\n",
        "                no_improvement_count = 0\n",
        "\n",
        "                # Save the best model\n",
        "                if save_path:\n",
        "                    self.save_model(save_path)\n",
        "            else:\n",
        "                no_improvement_count += 1\n",
        "                if no_improvement_count >= early_stopping_patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        return history\n",
        "\n",
        "def plot_improved_metrics(history, save_dir=None):\n",
        "    \"\"\"\n",
        "    Plot training metrics with directional accuracy\n",
        "    \"\"\"\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Plot closing price loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_close_loss'], label='Train Close Loss')\n",
        "    plt.plot(history['val_close_loss'], label='Val Close Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss (Closing Price)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'close_loss_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot directional accuracy\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['directional_accuracy'], label='Directional Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Up/Down Movement Prediction Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'directional_accuracy_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Other plots remain the same as before\n",
        "    # Plot full loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_full_loss'], label='Train Full Loss')\n",
        "    plt.plot(history['val_full_loss'], label='Val Full Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss (All Metrics)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'full_loss_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot MSE for closing price\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['close_mse'], label='Validation Close MSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('Validation Mean Squared Error (Closing Price)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'close_mse_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot R² for closing price\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['close_r2'], label='Validation Close R²')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('R²')\n",
        "    plt.title('Validation R² Score (Closing Price)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, 'close_r2_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(\n",
        "    embeddings_dir,\n",
        "    stock_csv_path,\n",
        "    output_dir=\"output_tinybert_lora_stock_prediction\",\n",
        "    metrics_count=6,  # Updated to 6 features\n",
        "    stocks_count=5,\n",
        "    epochs=50,\n",
        "    batch_size=16,\n",
        "    lr=3e-5,\n",
        "    lora_r=8,\n",
        "    lora_alpha=16\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a TinyBERT with LoRA model for stock prediction\n",
        "\n",
        "    Args:\n",
        "        embeddings_dir (str): Directory containing embeddings and dates files\n",
        "        stock_csv_path (str): Path to stock prices CSV file\n",
        "        output_dir (str): Directory to save outputs\n",
        "        metrics_count (int): Number of metrics per stock (now 6)\n",
        "        stocks_count (int): Number of stocks\n",
        "        epochs (int): Number of training epochs\n",
        "        batch_size (int): Batch size\n",
        "        lr (float): Learning rate\n",
        "        lora_r (int): LoRA rank\n",
        "        lora_alpha (int): LoRA alpha\n",
        "    \"\"\"\n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Paths to embeddings and dates files\n",
        "    train_embeddings_path = os.path.join(embeddings_dir, \"train_embeddings_9x312.pt\")\n",
        "    train_dates_path = os.path.join(embeddings_dir, \"train_dates.csv\")\n",
        "\n",
        "    val_embeddings_path = os.path.join(embeddings_dir, \"val_embeddings_9x312.pt\")\n",
        "    val_dates_path = os.path.join(embeddings_dir, \"val_dates.csv\")\n",
        "\n",
        "    test_embeddings_path = os.path.join(embeddings_dir, \"test_embeddings_9x312.pt\")\n",
        "    test_dates_path = os.path.join(embeddings_dir, \"test_dates.csv\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = StockDataset(\n",
        "        train_embeddings_path,\n",
        "        train_dates_path,\n",
        "        stock_csv_path,\n",
        "        metrics_count,\n",
        "        stocks_count\n",
        "    )\n",
        "\n",
        "    val_dataset = StockDataset(\n",
        "        val_embeddings_path,\n",
        "        val_dates_path,\n",
        "        stock_csv_path,\n",
        "        metrics_count,\n",
        "        stocks_count\n",
        "    )\n",
        "\n",
        "    test_dataset = StockDataset(\n",
        "        test_embeddings_path,\n",
        "        test_dates_path,\n",
        "        stock_csv_path,\n",
        "        metrics_count,\n",
        "        stocks_count\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = train_dataset.create_dataloader(batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = val_dataset.create_dataloader(batch_size=batch_size, shuffle=False)\n",
        "    test_dataloader = test_dataset.create_dataloader(batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if not all([train_dataloader, val_dataloader, test_dataloader]):\n",
        "        print(\"Error: Failed to create one or more dataloaders. Check data matching.\")\n",
        "        return None\n",
        "\n",
        "    # Output dimension is metrics_count * stocks_count\n",
        "    output_dim = metrics_count * stocks_count\n",
        "\n",
        "    # Create model\n",
        "    model = TinyBERTStockPredictor(\n",
        "        input_dim=312,  # From the embeddings\n",
        "        hidden_dim=312,  # TinyBERT hidden size\n",
        "        output_dim=output_dim,\n",
        "        lora_r=lora_r,\n",
        "        lora_alpha=lora_alpha\n",
        "    )\n",
        "\n",
        "    print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Input dimension: 312, Output dimension: {output_dim}\")\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Create scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = StockPredictionTrainer(\n",
        "        model,\n",
        "        optimizer,\n",
        "        metrics_count=metrics_count,\n",
        "        stocks_count=stocks_count,\n",
        "        device=device\n",
        "    )\n",
        "    trainer.set_scheduler(scheduler)\n",
        "\n",
        "    # Train model\n",
        "    print(f\"\\nTraining for {epochs} epochs...\")\n",
        "    history = trainer.train(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        epochs=epochs,\n",
        "        save_path=os.path.join(output_dir, 'best_model.pt'),\n",
        "        early_stopping_patience=10\n",
        "    )\n",
        "\n",
        "    # Plot training metrics\n",
        "    plot_metrics(history, save_dir=output_dir)\n",
        "\n",
        "    # Generate test predictions\n",
        "    print(\"\\nGenerating test predictions...\")\n",
        "    test_predictions = trainer.predict(test_dataloader)\n",
        "\n",
        "    # Save test predictions\n",
        "    torch.save(test_predictions, os.path.join(output_dir, 'test_predictions.pt'))\n",
        "    print(f\"Test predictions saved to {os.path.join(output_dir, 'test_predictions.pt')}\")\n",
        "\n",
        "    # Extract closing prices from test predictions\n",
        "    close_indices = [i * metrics_count + 3 for i in range(stocks_count)]\n",
        "    close_predictions = test_predictions[:, close_indices]\n",
        "\n",
        "    # Save close predictions separately\n",
        "    torch.save(close_predictions, os.path.join(output_dir, 'test_close_predictions.pt'))\n",
        "    print(f\"Close price predictions saved to {os.path.join(output_dir, 'test_close_predictions.pt')}\")\n",
        "\n",
        "    # Final test evaluation\n",
        "    test_metrics = trainer.validate(test_dataloader)\n",
        "    print(\"\\nFinal test metrics:\")\n",
        "    print(f\"Close Loss: {test_metrics['close_val_loss']:.6f}\")\n",
        "    print(f\"Close MSE: {test_metrics['close_mse']:.6f}\")\n",
        "    print(f\"Close R²: {test_metrics['close_r2']:.6f}\")\n",
        "\n",
        "    # Per-stock metrics\n",
        "    print(\"\\nPer-stock closing price metrics:\")\n",
        "    for stock_idx, (stock_name, metrics) in enumerate(test_metrics['stock_metrics'].items()):\n",
        "        stock_symbol = ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX'][stock_idx % 5]\n",
        "        print(f\"{stock_symbol}: MSE={metrics['mse']:.6f}, R²={metrics['r2']:.6f}\")\n",
        "\n",
        "    # Save metrics as CSV\n",
        "    stock_metrics_df = pd.DataFrame({\n",
        "        'Stock': ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX'],\n",
        "        'MSE': [test_metrics['stock_metrics'][f'stock_{i}']['mse'] for i in range(stocks_count)],\n",
        "        'R2': [test_metrics['stock_metrics'][f'stock_{i}']['r2'] for i in range(stocks_count)]\n",
        "    })\n",
        "    stock_metrics_df.to_csv(os.path.join(output_dir, 'stock_metrics.csv'), index=False)\n",
        "\n",
        "    # Save overall metrics\n",
        "    overall_metrics = {\n",
        "        'close_mse': test_metrics['close_mse'],\n",
        "        'close_r2': test_metrics['close_r2'],\n",
        "        'close_loss': test_metrics['close_val_loss'],\n",
        "        'full_loss': test_metrics['full_val_loss']\n",
        "    }\n",
        "\n",
        "    pd.DataFrame([overall_metrics]).to_csv(os.path.join(output_dir, 'overall_metrics.csv'), index=False)\n",
        "\n",
        "    return test_metrics\n",
        "\n",
        "def remap_targets_to_model_format(targets, output_dim=30):\n",
        "    \"\"\"\n",
        "    Remaps the targets from the CSV format to the model's expected format.\n",
        "\n",
        "    Original format is grouped by metric then company:\n",
        "    [AAPL_AdjClose, AMZN_AdjClose, GOOGL_AdjClose, META_AdjClose, NFLX_AdjClose,\n",
        "     AAPL_Close, AMZN_Close, GOOGL_Close, META_Close, NFLX_Close, ...etc]\n",
        "\n",
        "    Model expects format grouped by company then metric:\n",
        "    [AAPL_Open, AAPL_High, AAPL_Low, AAPL_Close, AAPL_AdjClose, AAPL_Volume,\n",
        "     AMZN_Open, AMZN_High, ...etc]\n",
        "\n",
        "    Args:\n",
        "        targets (torch.Tensor): Tensor of shape [batch_size, 30] in CSV order\n",
        "        output_dim (int): Expected output dimension (default 30)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Reordered tensor in model's expected format\n",
        "    \"\"\"\n",
        "    batch_size = targets.shape[0]\n",
        "\n",
        "    # Create a new tensor for reordered targets\n",
        "    reordered = torch.zeros((batch_size, output_dim), dtype=targets.dtype)\n",
        "\n",
        "    # CSV order of metrics\n",
        "    csv_metrics = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "    # Model expected order of metrics\n",
        "    model_metrics = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
        "\n",
        "    # Number of stocks and metrics\n",
        "    stocks_count = 5\n",
        "    metrics_count = 6\n",
        "\n",
        "    # Do the reordering\n",
        "    for stock_idx in range(stocks_count):\n",
        "        for model_metric_idx, model_metric in enumerate(model_metrics):\n",
        "            # Find the corresponding index in the CSV format\n",
        "            csv_metric_idx = csv_metrics.index(model_metric)\n",
        "\n",
        "            # Calculate source and target indices\n",
        "            src_idx = csv_metric_idx * stocks_count + stock_idx\n",
        "            tgt_idx = stock_idx * metrics_count + model_metric_idx\n",
        "\n",
        "            # Copy the values\n",
        "            reordered[:, tgt_idx] = targets[:, src_idx]\n",
        "\n",
        "    return reordered\n",
        "\n",
        "\n",
        "def improved_main():\n",
        "    \"\"\"\n",
        "    Main function with improved model and evaluation\n",
        "    \"\"\"\n",
        "    # Define paths to required data (same as before)\n",
        "    embeddings_dir = \"/content\"  # Base directory\n",
        "\n",
        "    # Embeddings files\n",
        "    train_embeddings_path = os.path.join(embeddings_dir, \"train_embeddings_9x312(3).pt\")\n",
        "    val_embeddings_path = os.path.join(embeddings_dir, \"val_embeddings_9x312(2).pt\")\n",
        "    test_embeddings_path = os.path.join(embeddings_dir, \"test_embeddings_9x312(1).pt\")\n",
        "\n",
        "    # Target CSV files\n",
        "    train_stocks_path = os.path.join(embeddings_dir, \"train.csv\")\n",
        "    val_stocks_path = os.path.join(embeddings_dir, \"val.csv\")\n",
        "    test_stocks_path = os.path.join(embeddings_dir, \"test.csv\")\n",
        "\n",
        "    # Output directory\n",
        "    output_dir = \"improved_tinybert_lora_stock_prediction\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Ensure the directories exist\n",
        "    os.makedirs(embeddings_dir, exist_ok=True)\n",
        "\n",
        "    # Check if data files exist (same checks as before)\n",
        "    required_files = [\n",
        "        train_embeddings_path,\n",
        "        val_embeddings_path,\n",
        "        test_embeddings_path,\n",
        "        train_stocks_path,\n",
        "        val_stocks_path,\n",
        "        test_stocks_path\n",
        "    ]\n",
        "\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "    if missing_files:\n",
        "        print(\"Error: The following required files are missing:\")\n",
        "        for file in missing_files:\n",
        "            print(f\" - {file}\")\n",
        "        print(\"\\nPlease ensure all required data files are available before running.\")\n",
        "        return\n",
        "\n",
        "    # Create datasets using the specific CSV format handler (same as before)\n",
        "    train_dataset = StockDataset(\n",
        "        train_embeddings_path,\n",
        "        train_stocks_path,\n",
        "        metrics_count=6,\n",
        "        stocks_count=5\n",
        "    )\n",
        "\n",
        "    val_dataset = StockDataset(\n",
        "        val_embeddings_path,\n",
        "        val_stocks_path,\n",
        "        metrics_count=6,\n",
        "        stocks_count=5\n",
        "    )\n",
        "\n",
        "    test_dataset = StockDataset(\n",
        "        test_embeddings_path,\n",
        "        test_stocks_path,\n",
        "        metrics_count=6,\n",
        "        stocks_count=5\n",
        "    )\n",
        "\n",
        "    # Create dataloaders with a custom collate function to reorder targets (same as before)\n",
        "    def collate_fn(batch):\n",
        "        inputs = torch.stack([item[0] for item in batch])\n",
        "        targets = torch.stack([item[1] for item in batch])\n",
        "        # Reorder targets to match model's expected format\n",
        "        reordered_targets = remap_targets_to_model_format(targets)\n",
        "        return inputs, reordered_targets\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        TensorDataset(train_dataset.embeddings, train_dataset.targets),\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        TensorDataset(val_dataset.embeddings, val_dataset.targets),\n",
        "        batch_size=16,\n",
        "        shuffle=False,  # Important to keep sequential for directional accuracy\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        TensorDataset(test_dataset.embeddings, test_dataset.targets),\n",
        "        batch_size=16,\n",
        "        shuffle=False,  # Important to keep sequential for directional accuracy\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    if not all([train_dataloader, val_dataloader, test_dataloader]):\n",
        "        print(\"Error: Failed to create one or more dataloaders. Check data matching.\")\n",
        "        return None\n",
        "\n",
        "    # Output dimension is metrics_count * stocks_count\n",
        "    output_dim = 6 * 5  # 6 metrics for 5 stocks\n",
        "\n",
        "    # Create improved model\n",
        "    model = ImprovedTinyBERTStockPredictor(\n",
        "        input_dim=312,  # From the embeddings\n",
        "        hidden_dim=312,  # TinyBERT hidden size\n",
        "        output_dim=output_dim,\n",
        "        lora_r=8,\n",
        "        lora_alpha=16\n",
        "    )\n",
        "\n",
        "    print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Input dimension: 312, Output dimension: {output_dim}\")\n",
        "\n",
        "    # Create optimizer with weight decay\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.05)\n",
        "\n",
        "    # Create scheduler - use CosineAnnealingLR instead of ReduceLROnPlateau\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=20, eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    # Create improved trainer\n",
        "    trainer = ImprovedStockPredictionTrainer(\n",
        "        model,\n",
        "        optimizer,\n",
        "        metrics_count=6,\n",
        "        stocks_count=5,\n",
        "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    )\n",
        "    trainer.set_scheduler(scheduler)\n",
        "\n",
        "    # Train model\n",
        "    print(f\"\\nTraining for 50 epochs...\")\n",
        "    history = trainer.train(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        epochs=50,\n",
        "        save_path=os.path.join(output_dir, 'best_model.pt'),\n",
        "        early_stopping_patience=10\n",
        "    )\n",
        "\n",
        "    # Plot training metrics with directional accuracy\n",
        "    plot_improved_metrics(history, save_dir=output_dir)\n",
        "\n",
        "    # Generate test predictions\n",
        "    print(\"\\nGenerating test predictions...\")\n",
        "    test_predictions = trainer.predict(test_dataloader)\n",
        "\n",
        "    # Save test predictions\n",
        "    torch.save(test_predictions, os.path.join(output_dir, 'test_predictions.pt'))\n",
        "    print(f\"Test predictions saved to {os.path.join(output_dir, 'test_predictions.pt')}\")\n",
        "\n",
        "    # Extract closing prices from test predictions\n",
        "    close_indices = [i * 6 + 3 for i in range(5)]  # Index 3 is Close in the model's order\n",
        "    close_predictions = test_predictions[:, close_indices]\n",
        "\n",
        "    # Save close predictions separately\n",
        "    torch.save(close_predictions, os.path.join(output_dir, 'test_close_predictions.pt'))\n",
        "    print(f\"Close price predictions saved to {os.path.join(output_dir, 'test_close_predictions.pt')}\")\n",
        "\n",
        "    # Final test evaluation\n",
        "    test_metrics = trainer.validate(test_dataloader)\n",
        "    print(\"\\nFinal test metrics:\")\n",
        "    print(f\"Close Loss: {test_metrics['close_val_loss']:.6f}\")\n",
        "    print(f\"Close MSE: {test_metrics['close_mse']:.6f}\")\n",
        "    print(f\"Close R²: {test_metrics['close_r2']:.6f}\")\n",
        "    print(f\"Directional Accuracy: {test_metrics.get('directional_accuracy', float('nan')):.4f}\")\n",
        "\n",
        "    # Per-stock metrics\n",
        "    print(\"\\nPer-stock closing price metrics:\")\n",
        "    stock_symbols = ['AAPL', 'AMZN', 'GOOGL', 'META', 'NFLX']\n",
        "    for stock_idx, (stock_name, metrics) in enumerate(test_metrics['stock_metrics'].items()):\n",
        "        stock_symbol = stock_symbols[stock_idx % 5]\n",
        "        print(f\"{stock_symbol}: MSE={metrics['mse']:.6f}, R²={metrics['r2']:.6f}, \"\n",
        "              f\"Dir Acc={metrics.get('dir_accuracy', float('nan')):.4f}\")\n",
        "\n",
        "    # Save directional accuracy metrics separately\n",
        "    dir_acc_metrics = {\n",
        "        'overall': test_metrics.get('directional_accuracy', float('nan')),\n",
        "        'stocks': {\n",
        "            stock: metrics.get('dir_accuracy', float('nan'))\n",
        "            for stock, (_, metrics) in zip(stock_symbols, test_metrics['stock_metrics'].items())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    pd.DataFrame([dir_acc_metrics['overall']], columns=['DirectionalAccuracy']).to_csv(\n",
        "        os.path.join(output_dir, 'directional_accuracy.csv'), index=False\n",
        "    )\n",
        "\n",
        "    pd.DataFrame({\n",
        "        'Stock': stock_symbols,\n",
        "        'DirectionalAccuracy': [dir_acc_metrics['stocks'].get(stock, float('nan')) for stock in stock_symbols]\n",
        "    }).to_csv(os.path.join(output_dir, 'stock_directional_accuracy.csv'), index=False)\n",
        "\n",
        "    return test_metrics\n",
        "\n",
        "\n",
        "# Replace the main function call with improved version\n",
        "if __name__ == \"__main__\":\n",
        "    improved_main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISWZiquibAqA",
        "outputId": "939af376-f223-4582-c323-d16dc35c40a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading embeddings from /content/train_embeddings_9x312(3).pt\n",
            "Loading stock data from /content/train.csv\n",
            "Loading stock price data from /content/train.csv...\n",
            "CSV structure sample:\n",
            "   Unnamed: 0  Adj Close  Adj Close.1  Adj Close.2  Adj Close.3  Adj Close.4  \\\n",
            "0  2018-01-09  40.966282    62.634998    55.374752   186.988708   209.309998   \n",
            "\n",
            "     Close    Close.1  Close.2     Close.3  ...     Open.1     Open.2  \\\n",
            "0  43.5825  62.634998  55.6395  187.869995  ...  62.845001  55.922001   \n",
            "\n",
            "       Open.3      Open.4      Volume    Volume.1    Volume.2    Volume.3  \\\n",
            "0  188.699997  212.110001  86336000.0  73226000.0  26808000.0  12393100.0   \n",
            "\n",
            "    Volume.4  is_business_day  \n",
            "0  6125900.0             True  \n",
            "\n",
            "[1 rows x 32 columns]\n",
            "CSV columns: ['Unnamed: 0', 'Adj Close', 'Adj Close.1', 'Adj Close.2', 'Adj Close.3', 'Adj Close.4', 'Close', 'Close.1', 'Close.2', 'Close.3', 'Close.4', 'High', 'High.1', 'High.2', 'High.3', 'High.4', 'Low', 'Low.1', 'Low.2', 'Low.3', 'Low.4', 'Open', 'Open.1', 'Open.2', 'Open.3', 'Open.4', 'Volume', 'Volume.1', 'Volume.2', 'Volume.3', 'Volume.4', 'is_business_day']\n",
            "Creating target tensors...\n",
            "Created 340 targets with shape torch.Size([340, 30])\n",
            "Warning: Number of targets (340) doesn't match number of embeddings (345)\n",
            "Truncated to 340 matching examples\n",
            "Loading embeddings from /content/val_embeddings_9x312(2).pt\n",
            "Loading stock data from /content/val.csv\n",
            "Loading stock price data from /content/val.csv...\n",
            "CSV structure sample:\n",
            "   Unnamed: 0  Adj Close  Adj Close.1  Adj Close.2  Adj Close.3  Adj Close.4  \\\n",
            "0  2018-03-13  42.464058    79.408997      56.7243   181.026825   315.880005   \n",
            "\n",
            "     Close    Close.1    Close.2     Close.3  ...     Open.1     Open.2  \\\n",
            "0  44.9925  79.408997  56.995499  181.880005  ...  80.797997  58.591499   \n",
            "\n",
            "       Open.3      Open.4       Volume     Volume.1    Volume.2    Volume.3  \\\n",
            "0  185.610001  323.869995  126774000.0  130638000.0  43140000.0  18067500.0   \n",
            "\n",
            "     Volume.4  is_business_day  \n",
            "0  12917200.0             True  \n",
            "\n",
            "[1 rows x 32 columns]\n",
            "CSV columns: ['Unnamed: 0', 'Adj Close', 'Adj Close.1', 'Adj Close.2', 'Adj Close.3', 'Adj Close.4', 'Close', 'Close.1', 'Close.2', 'Close.3', 'Close.4', 'High', 'High.1', 'High.2', 'High.3', 'High.4', 'Low', 'Low.1', 'Low.2', 'Low.3', 'Low.4', 'Open', 'Open.1', 'Open.2', 'Open.3', 'Open.4', 'Volume', 'Volume.1', 'Volume.2', 'Volume.3', 'Volume.4', 'is_business_day']\n",
            "Creating target tensors...\n",
            "Created 44 targets with shape torch.Size([44, 30])\n",
            "Loading embeddings from /content/test_embeddings_9x312(1).pt\n",
            "Loading stock data from /content/test.csv\n",
            "Loading stock price data from /content/test.csv...\n",
            "CSV structure sample:\n",
            "   Unnamed: 0  Adj Close  Adj Close.1  Adj Close.2  Adj Close.3  Adj Close.4  \\\n",
            "0  2018-03-19  41.362183    77.246498    54.741783   171.750534   313.480011   \n",
            "\n",
            "       Close    Close.1    Close.2     Close.3  ...     Open.1  Open.2  \\\n",
            "0  43.825001  77.246498  55.003502  172.559998  ...  77.726501  55.888   \n",
            "\n",
            "       Open.3      Open.4       Volume     Volume.1    Volume.2    Volume.3  \\\n",
            "0  177.009995  315.799988  133787200.0  131616000.0  63656000.0  88140100.0   \n",
            "\n",
            "    Volume.4  is_business_day  \n",
            "0  9925200.0             True  \n",
            "\n",
            "[1 rows x 32 columns]\n",
            "CSV columns: ['Unnamed: 0', 'Adj Close', 'Adj Close.1', 'Adj Close.2', 'Adj Close.3', 'Adj Close.4', 'Close', 'Close.1', 'Close.2', 'Close.3', 'Close.4', 'High', 'High.1', 'High.2', 'High.3', 'High.4', 'Low', 'Low.1', 'Low.2', 'Low.3', 'Low.4', 'Open', 'Open.1', 'Open.2', 'Open.3', 'Open.4', 'Volume', 'Volume.1', 'Volume.2', 'Volume.3', 'Volume.4', 'is_business_day']\n",
            "Creating target tensors...\n",
            "Created 42 targets with shape torch.Size([42, 30])\n",
            "Warning: Number of targets (42) doesn't match number of embeddings (43)\n",
            "Truncated to 42 matching examples\n",
            "Model created with 14507846 parameters\n",
            "Input dimension: 312, Output dimension: 30\n",
            "\n",
            "Training for 50 epochs...\n",
            "Epoch 1/50 - Train Close Loss: 29214237.818182, Val Close Loss: 27918237.333333, Close MSE: 0.039025, Close R²: -161.138214, Dir Acc: 0.4875\n",
            "Model saved to improved_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 2/50 - Train Close Loss: 29068626.727273, Val Close Loss: 27918237.333333, Close MSE: 0.039025, Close R²: -161.217987, Dir Acc: 0.4646\n",
            "Epoch 3/50 - Train Close Loss: 28763614.727273, Val Close Loss: 27918237.333333, Close MSE: 0.039025, Close R²: -161.134064, Dir Acc: 0.4750\n",
            "Epoch 4/50 - Train Close Loss: 28826043.909091, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.844757, Dir Acc: 0.5125\n",
            "Model saved to improved_tinybert_lora_stock_prediction/best_model.pt\n",
            "Epoch 5/50 - Train Close Loss: 29166486.818182, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.564438, Dir Acc: 0.5208\n",
            "Epoch 6/50 - Train Close Loss: 28405098.545455, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.607864, Dir Acc: 0.5229\n",
            "Epoch 7/50 - Train Close Loss: 28720555.272727, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.337280, Dir Acc: 0.4938\n",
            "Epoch 8/50 - Train Close Loss: 28899739.454545, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.569031, Dir Acc: 0.5417\n",
            "Epoch 9/50 - Train Close Loss: 28816482.454545, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.429932, Dir Acc: 0.5104\n",
            "Epoch 10/50 - Train Close Loss: 28444501.636364, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.374161, Dir Acc: 0.5042\n",
            "Epoch 11/50 - Train Close Loss: 28700937.545455, Val Close Loss: 27918237.333333, Close MSE: 0.039025, Close R²: -160.542389, Dir Acc: 0.5083\n",
            "Epoch 12/50 - Train Close Loss: 29082535.363636, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.577026, Dir Acc: 0.4958\n",
            "Epoch 13/50 - Train Close Loss: 28548796.818182, Val Close Loss: 27918236.666667, Close MSE: 0.039025, Close R²: -160.477051, Dir Acc: 0.4875\n",
            "Epoch 14/50 - Train Close Loss: 28922714.727273, Val Close Loss: 27918237.333333, Close MSE: 0.039025, Close R²: -160.563156, Dir Acc: 0.4875\n",
            "Early stopping at epoch 14\n",
            "\n",
            "Generating test predictions...\n",
            "Test predictions saved to improved_tinybert_lora_stock_prediction/test_predictions.pt\n",
            "Close price predictions saved to improved_tinybert_lora_stock_prediction/test_close_predictions.pt\n",
            "\n",
            "Final test metrics:\n",
            "Close Loss: 32230114.000000\n",
            "Close MSE: 0.048596\n",
            "Close R²: -268.719299\n",
            "Directional Accuracy: 0.5350\n",
            "\n",
            "Per-stock closing price metrics:\n",
            "AAPL: MSE=0.242980, R²=-8.337715, Dir Acc=0.6341\n",
            "AMZN: MSE=0.000000, R²=-339.013519, Dir Acc=0.5122\n",
            "GOOGL: MSE=0.000000, R²=-336.585510, Dir Acc=0.3902\n",
            "META: MSE=0.000000, R²=-375.559753, Dir Acc=0.3659\n",
            "NFLX: MSE=0.000000, R²=-284.100037, Dir Acc=0.3415\n"
          ]
        }
      ]
    }
  ]
}